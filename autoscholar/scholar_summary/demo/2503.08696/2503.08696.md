# Multimodal Stock Price Prediction: A Case Study of the Russian Securities Market

Kasymkhan Khubiyev Sirius University of Science and Technology, Sirius, Russia kasymkhankhubievnis@gmail.com

Mikhail Semenov Sirius University of Science and Technology, Sirius, Russia semenov.me@talantiuspeh.ru

March 13, 2025

#### Abstract

Classical asset price forecasting methods primarily rely on numerical data, such as price time series, trading volumes, limit order book data, and technical analysis indicators. However, the news flow plays a significant role in price formation, making the development of multimodal approaches that combine textual and numerical data for improved prediction accuracy highly relevant.

This paper addresses the problem of forecasting financial asset prices using the multimodal approach that combines candlestick time series and textual news flow data. A unique dataset was collected for the study, which includes time series for 176 Russian stocks traded on the Moscow Exchange and 79, 555 financial news articles in Russian.

For processing textual data, pre-trained models RuBERT and Vikhr-Qwen2.5-0.5b-Instruct (a large language model) were used, while time series and vectorized text data were processed using an LSTM recurrent neural network. The experiments compared models based on a single modality (time series only) and two modalities, as well as various methods for aggregating text vector representations.

Prediction quality was estimated using two key metrics: Accuracy (direction of price movement prediction: up or down) and Mean Absolute Percentage Error (MAPE), which measures the deviation of the predicted price from the true price. The experiments showed that incorporating textual modality reduced the MAPE value by 55%.

The resulting multimodal dataset holds value for the further adaptation of language models in the financial sector. Future research directions include optimizing textual modality parameters, such as the time window, sentiment, and chronological order of news messages.

### 1 Introduction

Building a price forecast for an asset is a crucial task for financial market participants, as it enables strategic planning, optimal investment portfolio management, and risk assessment. Numerous attempts have been made to apply machine learning methods to construct such forecasts [\[3,](#page-14-0) [1,](#page-14-1) [2\]](#page-14-2). With the growing popularity of deep learning models, researchers have shifted their focus toward the application of neural networks. At the same time, the problem of accurately accounting for the news flow as a key factor influencing market behavior is being reconsidered with the rapid development of generative artificial intelligence models and large language models (LLMs) such as ChatGPT, FinGPT, GigaChat, LLama, and others. In financial economics, LLMs are still rarely used, and their full potential remains untapped.

<sup>0</sup>Preprint for the Program Systems: Theory and Applications journal

Researchers are exploring the use of natural language processing models to enhance the accuracy of asset price forecasts and investment portfolio management strategies.

The study [\[4\]](#page-14-3) describes the use of sentiment analysis of news as an additional parameter. The authors employed the FinBert model, trained on financial data, to assess the sentiment of news articles as positive, negative, or neutral. The study utilized time series data from candlestick charts of the U. S. stock market index, Standard & Poor's 500 (S&P 500). A machine learning model — random forest —was used for price prediction. The study concluded that incorporating sentiment analysis of news flow improves prediction accuracy.

In the study [\[5\]](#page-14-4), the authors aimed to develop a multimodal artificial intelligence model capable of providing well-founded and accurate forecasts for time series data. They implemented a model that generates predictions of an asset's monthly or weekly returns, accompanied by a textual explanation from a language model based on the user's input query.

The study [\[6\]](#page-14-5) proposed an approach for fine-tuning instructions to interpret numerical values and contextualize financial data. Kulikova et al. [\[7\]](#page-14-6) examined the effect of classifying news into thematic groups. The authors demonstrated that, in most cases, it is advisable to use a single thematic group of news for the deep learning models considered (Temporal Convolutional Network, D-Linear, Transformer, and Temporal Fusion Transformer). They also determined the probabilities of forecast improvement for the 20 thematic groups analyzed.

In all the aforementioned studies, the models were implemented using a multimodal approach for the U. S. stock market, with English as the modality language. Notably, the news flow was not integrated directly into the predictor's input vector but rather through a preprocessing block in the form of an additional parameter, such as sentiment analysis, news frequency related to the asset, or news classification, etc.

The objective of the current study is to demonstrate the advantages of a new multimodal method over predictions based solely on numerical data and to present a Russian-language financial news dataset.

To achieve this objective, we formulated the following key tasks:

- 1. Construct a multimodal dataset consisting of time series data and news articles.
- 2. Develop a predictive model capable of utilizing one or two modalities.
- 3. Train the predictive model and analyze the values of accuracy functions and metrics, specifically Accuracy and MAPE.

In this study, we propose a new multimodal approach for integrating news flow into time series numerical data. The text of the news articles is converted into a vector representation and fed into the model alongside the time series vector.

Our hypothesis is that the multimodal approach will enable predictive models to extract semantic information from the text, thereby improving the accuracy of asset price forecasts.

### 2 Data Collection and Structuring

Multimodality implies the use of more than one data modality, which affects both the data structure and the logic of predictive model development. We utilize two types of modalities: (a) numerical — time series of stock prices, and (b) textual — news streams. To train the predictive model and analyze its performance, we collected an original dataset.

The time series, represented as candlestick data with open, close, high, and low prices, were obtained through the Algopack API of the Moscow Exchange (MOEX). For the numerical experiment, we selected stock time series data spanning from July 7, 2022, to August 30, 2024, covering 176 companies. During this period, the Russian stock market experienced phases of rapid growth and decline, with the IMOEX index rising from 2,213.81 to 2,650.32 points (+19, 72%).

We collected 79, 555 news articles from various sources, including the online publication "RBC" (1,823 articles), "BCS Express" (11,331), and "BCS Technical Analysis" (9,670), the investment company website "Finam" (20,647), the trader community website "SmartLab.ru" (30,857), as well as the Telegram channel "RDV" (5,227).

Several factors justify the selection of these sources. First, they provide news coverage for the required time period. Second, the institutional differences between sources, along with variations in writing style and levels of expertise, contribute to a more objective representation of events related to the analyzed time series.

News messages were tokenized using two models: RuBERT [\[8\]](#page-14-7) and Vikhr-Qwen2.5-0.5b-Instruct [\[9\]](#page-14-8) (further as Qwen). In the context of tokenized text, a word refers to a token – an element of the vector space represented as an index in the tokenizer's vocabulary. Descriptive statistics of the dataset (in tokens), including mean, standard deviation, minimum, maximum word count, and quartiles, are presented in Tables [1](#page-2-0) and [2.](#page-2-1) It is important to note that tokenization can increase the word count in a text, for example, by splitting words into smaller components.

| Source                 | Mean | Std | Min | Max | Q25 | Q50 | Q75 |
|------------------------|------|-----|-----|-----|-----|-----|-----|
| RDV                    | 134  | 88  | 8   | 512 | 65  | 123 | 187 |
| Finam                  | 221  | 135 | 18  | 512 | 116 | 178 | 284 |
| BCS Express            | 20   | 10  | 4   | 82  | 13  | 17  | 26  |
| BCS Technical Analysis | 502  | 37  | 29  | 512 | 512 | 512 | 512 |
| RBC                    | 43   | 7   | 16  | 75  | 39  | 44  | 48  |
| SmartLab               | 21   | 8   | 5   | 82  | 15  | 19  | 25  |

<span id="page-2-0"></span>Table 1: Statistical features of the dataset after tokenization, RuBert.

| Source                 | Mean | Std | Min | Max  | Q25  | Q50  | Q75  |
|------------------------|------|-----|-----|------|------|------|------|
| RDV                    | 215  | 157 | 3   | 1324 | 92   | 187  | 304  |
| Finam                  | 453  | 405 | 35  | 5732 | 211  | 319  | 501  |
| BCS Express            | 36   | 19  | 5   | 163  | 23   | 32   | 47   |
| BCS Technical Analysis | 1493 | 310 | 40  | 2221 | 1448 | 1545 | 1665 |
| RBC                    | 75   | 12  | 28  | 105  | 68   | 77   | 83   |
| SmartLab               | 33   | 12  | 7   | 120  | 25   | 31   | 39   |

<span id="page-2-1"></span>Table 2: Statistical features of the dataset after tokenization, Qwen.

Table [3](#page-2-2) provides examples of how a phrase changes after tokenization. For instance, the word «открывает» is split into three subcomponents: «от», «##к», and «##рывает», where the «##» prefix indicates that the token is a continuation of the previous token.

| Original text               | Tokenized text                          |
|-----------------------------|-----------------------------------------|
| Доллар снова ниже 69 рублей | До ##лла ##р снова ниже 69 рублей       |
| Москвич банкрот?            | Москви ##ч банк ##рот ?                 |
| НПО Наука Отчет РСБУ        | Н ##П, ##О Наука От ##чет Р ##С ##Б ##У |
| T-банк это желтый банк      | T - банк это же ##лт ##ый банк          |

<span id="page-2-2"></span>Table 3: Original and tokenized texts examples

News articles characteristics On the "BCS Technical Analysis" platform, news articles tend to be lengthy, which imposes limitations on tokenizers. Specifically, as shown in Tables [1](#page-2-0) and [2,](#page-2-1) the RuBERT model truncates the tokenized vector for longer texts. Additionally, the

| Source                 | Article fragment (heading)                     | Tags                          |
|------------------------|------------------------------------------------|-------------------------------|
| RDV                    | Сегежа (SGZH): таргет 16.2 руб., апсайд +102   | SGZH                          |
| RDV                    | Артген биотех (ABIO) завершил доклинические    | аналитика, ABIO               |
| Finam                  | Индекс МосБиржи восстанавливает позиции и приб | ФосАгро, ВСМПО-АВСМ, CNYRUB   |
| Finam                  | «Ашинский метзавод» назвал АО "Урал-ВК" своим  | АшинскийМЗ                    |
| BCS Express            | «Восходящее окно»: в каких бумагах замечен это | Селигдар SELG, ЕвроТранс EUTR |
| BCS Express            | «Сила Сибири» выйдет на максимальную мощность  | Газпром GAZP                  |
| BCS Technical Analysis | Мечел. Что ждать от бумаг на следующей неделе  | Мечел                         |
| BCS Technical Analysis | На предыдущей торговой сессии акции Норникеля  | ГМК Норникель                 |

<span id="page-3-0"></span>Table 4: Examples of news articles (header snippet) and assigned tags

average length of tokenized text using the Qwen model exceeds that of RuBERT, indicating that Qwen has a broader vocabulary and a stronger text decomposition capability.

Furthermore, we collected data on 176 companies, forming a dataset consisting of tuples in the format:

<sup>≪</sup>ticker - company name - company activity description≫.

Such data are essential in our case for: (a) extracting keywords from company descriptions, and (b) improving the language model's ability to link events described in news articles to specific companies and assess the impact of news on price dynamics.

The dataset of news articles includes the following parameters: publication date, source, title, article body, and tags (keywords). For sources such as "RDV" and "SmartLab", article titles are absent, and the corresponding fields are filled with a label: no title. In our case, tags may include the full or abbreviated company name along with the corresponding ticker, the name of the market sector, and similar information. Tags in news articles were assigned by the article authors.

For the "RDV" source, tags were marked by authors in the form of hashtags (e. g., #цифры, #аналитика). In "BCS Express" and "BCS Technical Analysis", tags were specified in dedicated fields at the beginning or end of the news article (e. g., PhoseAgro, Russian market) and were extracted from the HTML code of the page using the corresponding HTML tags. When tags were absent ("RBC", "SmartLab"), the parameter in the dataset remained empty.

Table [4](#page-3-0) provides examples of news articles (headline fragments) along with their assigned tags.

### 3 Methodology

To validate our hypothesis regarding the advantages of the multimodal approach, we have planned a series of experiments.

The first series of experiments focused on predicting prices using only numerical time series of candlestick characteristics (close, open, high, and low prices). The quality metrics obtained from this experiment serve as baseline values against which improvements in price prediction accuracy using the proposed multimodal approach will be evaluated.

The second series of experiments aims to generate predictions and compute accuracy metrics (Accuracy, MAPE) using the multimodal approach while exploring different aggregation methods (Sum, Mean) for the vectorized news stream.

#### 3.1 The Single-Modality Approach

We first conducted a series of experiments on asset price prediction using only time series data. For this, we applied classical machine learning models to the daily price values (close, open, high, low), including linear regression (LinReg), k-nearest neighbors (KNN), decision tree (DT), random forest (RF), and the boosting algorithm XGBoost (XGB). Among deep learning models, we utilized a long short-term memory recurrent neural network (LSTM).

Conceptually, the experiment consists of two tasks: (a) predicting the price movement direction (increase or decrease), which is a binary classification task; (b) predicting the actual price, which is a regression task.

At this stage of the experiment, 176 companies were grouped into 23 industry sectors. We randomly selected 9 economic sectors and, within each sector, randomly chose two companies. Table [5](#page-4-0) lists the selected sectors and companies (tickers) that participated in the computational experiment.

Table [6](#page-4-1) provides statistical data on the closing price time series of the selected assets. The correlation heat map of the closing price time series is shown in Figure [1.](#page-5-0) An interesting feature of the examined period is that the market underwent two phase shifts — from a general price decline to growth and back again — as indicated by the vertical lines in Figure [2.](#page-6-0)

| Company (ticker)                          |
|-------------------------------------------|
| Mechel (MLTR), TMK-Group (TRMK)           |
| Surgutneftegas (SNGS), Gaspromneft (SIBN) |
| Magnit (MGNT), Lenta (LENT)               |
| PIK (PIKK), Samolet (SMLT)                |
| MTS (MTSS), Rostelecom (RTKMP)            |
| AEROFLOT (AFLT), Sovcomflot (FLOT)        |
| Bank Saint-Petersburg (BSPB), SFI (SFIN)  |
| Phosagro (PHOR), Kazanorgsintez (KZOSP)   |
| Rushydro (HYDR), Rosseti Center (MRKC)    |
|                                           |

<span id="page-4-0"></span>Table 5: Economic sectors and companies (tickers) included into the dataset

| Ticker | Mean      | Std       | Min       | Max       | Q25       | Q50       | Q75       |
|--------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| MTLR   | 191.8245  | 72.5652   | 81.2800   | 332.8800  | 123.8500  | 187.6700  | 251.6400  |
| TRMK   | 153.1245  | 64.9362   | 55.8200   | 271.0000  | 87.1400   | 166.4200  | 218.7800  |
| SNGS   | 27.0104   | 4.0119    | 17.3500   | 36.9600   | 23.7750   | 27.3300   | 30.0250   |
| SIBN   | 601.5097  | 163.9205  | 335.5500  | 934.2500  | 452.0500  | 582.6500  | 748.9000  |
| MGNT   | 5691.6429 | 1161.7684 | 4040.0000 | 8444.0000 | 4665.0000 | 5495.0000 | 6375.0000 |
| LENT   | 814.3870  | 154.9502  | 650.0000  | 1263.0000 | 716.5000  | 749.0000  | 843.5000  |
| PIKK   | 732.6617  | 94.8650   | 518.0000  | 955.5000  | 656.7000  | 732.9000  | 811.5000  |
| SMLT   | 3120.8996 | 594.1018  | 1926.5000 | 4145.5000 | 2572.0000 | 3045.0000 | 3713.0000 |
| MTSS   | 264.5382  | 32.0791   | 183.0000  | 346.9500  | 239.0000  | 266.2500  | 289.7500  |
| RTKMP  | 68.1797   | 9.2753    | 52.2500   | 92.1000   | 60.4500   | 68.0000   | 74.7000   |
| AFLT   | 38.1316   | 10.3131   | 22.4400   | 64.4000   | 27.9700   | 38.8800   | 44.1200   |
| FLOT   | 88.0111   | 39.5834   | 29.9200   | 149.3000  | 42.1000   | 97.2000   | 124.1800  |
| BSPB   | 211.1501  | 101.2533  | 67.5700   | 387.6800  | 100.8400  | 210.9900  | 295.3400  |
| SFIN   | 762.9939  | 428.5679  | 425.8000  | 1975.0000 | 497.4000  | 518.0000  | 992.0000  |
| PHOR   | 6774.6040 | 618.1977  | 4997.0000 | 8153.0000 | 6416.0000 | 6763.0000 | 7278.0000 |
| KZOSP  | 25.8603   | 5.2029    | 15.3500   | 40.5700   | 21.9400   | 27.0700   | 29.8500   |
| HYDR   | 0.7697    | 0.0810    | 0.5178    | 1.0278    | 0.7318    | 0.7721    | 0.8210    |
| MRKS   | 0.5247    | 0.2382    | 0.2025    | 1.0745    | 0.2735    | 0.5550    | 0.7475    |

<span id="page-4-1"></span>Table 6: Descriptive characteristics for company shares

To evaluate prediction quality in the classification task, we used the Accuracy metric, while for regression, we employed MAPE (Mean Absolute Percentage Error). The choice of these metrics is justified by the nature of the tasks. In classification, the model must accurately

![](_page_5_Figure_0.jpeg)

<span id="page-5-0"></span>Figure 1: The correlations heatmap for 18 assets (close price).

predict the price movement direction either an increase (denoted by "+") or a decrease (denoted by "−"). The MAPE metric is best suited for assessing regression quality within the financial domain: it represents the average deviation from the asset's actual price in percentage terms, making it easily interpretable in monetary value.

Figure [3](#page-6-1) illustrates the model development process for utilizing one and two modalities.

As the input parameter, the model received a return vector of the asset, calculated based on the closing price (close) over the previous five trading sessions:

<span id="page-5-1"></span>
$$Return(d+1) = \frac{close(d+1)}{close(d)} - 1.\tag{1}$$

The model's output was a prediction for the next trading session.

To assess the accuracy of predicting the price movement direction, the predicted class was determined by the sign (±) of the forecasted return value, as the return of an asset represents the relative rate of change. Thus, a positive return indicates a price increase, while a negative return signifies a decline. To evaluate the quality of the asset price forecast, the predicted return vector was converted into price (in Russian rubles):

<span id="page-5-2"></span>
$$price(d+1) = (Return(d+1) + 1) \cdot price(d). \tag{2}$$

The pointwise predicted price vector, obtained through transformation, was compared to the historical price vector of assets using the MAPE metric.

The choice of return (rather than price) as the target variable for the predictive model is justified by the fact that when prices exceed historical highs (or fall below historical lows) during market growth (or decline), the applicability of traditional methods becomes limited.

![](_page_6_Figure_0.jpeg)

<span id="page-6-0"></span>Figure 2: Normalized close prices of assets. Market phase transition dates denoted by vertical dashed lines.

![](_page_6_Figure_2.jpeg)

<span id="page-6-1"></span>Figure 3: Pipeline for a single and dual modalities models.

Based on this reasoning, candlestick characteristics (close, open, high, and low prices) were considered in the form of relative price changes, calculated using a formula similar to Eq. [\(1\)](#page-5-1).

Next, a rolling window of five trading days was applied to the relative price changes to form a vector-row, which was then fed into the predictive model. As a result, the model receives a vector of 20 parameters as input and predicts a single output value — the return of the instrument at the end of the next trading session.

#### 3.2 The Dual-Modality Approach

For the experiment involving news flow, we selected news articles relevant to the analyzed assets based on keyword matching (Table [5\)](#page-4-0). The keywords were chosen as the top 30 words extracted using the TF-IDF method. This method determines the importance of words in a text by considering their frequency of occurrence and uniqueness across the entire corpus. An example of keywords extracted using TF-IDF is presented in Table [7.](#page-7-0)

After obtaining the list of keywords using the TF-IDF method, we further expanded it with the help of the ChatGPT-4o model. This allowed us to increase keyword variability through permutations, letter substitutions, and modifications of word endings (Table [8\)](#page-7-1). The selected

| Ticker | Keywords                                                                                    |
|--------|---------------------------------------------------------------------------------------------|
| MTLR   | mechel, mining, ore, raw materials, energy, ferroalloys, coal                               |
| SNGS   | gas, geological exploration, oil, Surgutneftegas, petroleum products, electricity, drilling |
| SMLT   | rent, development, developer, real estate, construction, Moscow region, residential areas   |

<span id="page-7-0"></span>Table 7: Keywords by companies extracted from their descriptions

| Ticker | Keywords                                                                       |
|--------|--------------------------------------------------------------------------------|
| MTLR   | мечел, метчел, мечал, mechel, Mchel, ферросплавы, фурросплав                   |
| SNGS   | сургутнефтегаз, surgutneftegaz, surgut, сурнефтегаз, сургаз, cургут, сур-нфтгз |
| SMLT   | самолет, smlt, samolet, samalet, Самлет                                        |

<span id="page-7-1"></span>Table 8: Complementary keywords generated.

news articles for each company (ticker) were converted into vectors and filtered to remove duplicates. Figure [4](#page-7-2) presents a distribution chart of the news articles for the companies after filtration. As a vectorizer for the Russian language news stream, we employed two models: RuBERT [\[8\]](#page-14-7) and Qwen [\[9\]](#page-14-8).

![](_page_7_Figure_5.jpeg)

<span id="page-7-2"></span>Figure 4: The distribution of news articles by company after filtration (in percents).

While working with the news stream, we encountered two main challenges. The first challenge is the problem of news rewriting, which necessitates filtering out duplicate articles. To ensure that our model accounts for each news article only once, it is essential to implement a duplicate identification algorithm. The second challenge is to determinate an asset on which is affected the news article. This problem can be framed as a classification task, where tickers serve as class labels.

To address the issue of news rewriting, we designed a Siamese neural network. We constructed a training dataset using the GigaChat API as follows: for each article, three paraphrased versions of both the title and body were generated. Then, pairs were randomly formed in equal proportion from the original and paraphrased news articles and their titles. The Siamese neural network was designed as follows: a pair of news articles is fed as input, and vector representations of the articles are extracted using the RuBERT model [\[8\]](#page-14-7). The two vectors are then concatenated, and the resulting vector is passed through a fully connected neural network (MLP). To determine the optimal depth of the MLP model, we conducted a series of experiments, evaluating both prediction accuracy and news stream processing time. Based on the results, we selected the MLP architecture with three layers. The filtered news articles are then converted into vectors so that duplicate classification can be performed in a one-shot mode when new articles arrive. This approach reduces both the processing time of the news stream and the computational resources required (in our case, a GPU V100).

To address the second challenge – matching news article samples by date and utilizing them for price forecasting – it is essential to formalize the data selection and prediction process. We assume that the closing price prediction for an asset is made for each trading day at the market opening. In this case, only news articles published before the start of the current trading day are included in the dataset.

The dataset is formed by grouping news articles based on their publication date. For predicting the price on a given day, only articles published on the previous trading day are used. For example, analytical articles such as those under the "Technical Analysis" section from the "BCS" source, which are published daily before the market opens, are included in the dataset for forecasting the prices of assets analyzed in those reports. This approach ensures that the most relevant information is considered, thereby improving prediction accuracy.

For the dual-modality approach, training sequences were formed by concatenating price return vectors from the previous five days with news stream vectors. The relative price return vectors were constructed similarly to the single-modality experiment, while news articles were selected from the previous trading day based on the chosen asset. These news articles were then transformed into vectors and aggregated.

If no publications were available on the previous day or before the market opened on the current day, a zero vector was concatenated with the relative price return vector of length 768 for the RuBERT model and 896 for the Vikhr-Qwen2.5-0.5b-Instruct (Qwen) model. Otherwise, the aggregated news vector of the same length was appended. These final vector lengths correspond to the output sizes of the pretrained RuBERT and Qwen models.

In this study, we explored two approaches for aggregating news vectors: vector summation (Sum) and averaged summation (Mean). By vector summation, we mean summing the values of corresponding vector coordinates. In the averaged summation approach, each coordinate of the aggregated vector is assigned the arithmetic mean of the corresponding coordinates across all aggregated vectors. The baseline RuBERT model has a limited context window of 512 tokens. As a result, articles exceeding this limit were either truncated or split for separate processing, meaning that a single news article could correspond to multiple vectors. In contrast, the Qwen model has a significantly larger context window of 32,768 tokens (64 times larger), allowing it to process entire articles without truncation. Next, we compare how different news vectorization methods impact the accuracy of price predictions.

The pointwise predicted return vectors were converted into asset prices using equation [\(2\)](#page-5-2). The prediction quality was evaluated using two metrics: Accuracy and Mean Absolute Percentage Error (MAPE). Accuracy was measured as the proportion of correctly predicted signs of the return vector elements—either positive or negative. The MAPE metric indicates the average percentage deviation of the predicted price from the actual value. This allows us to assess the prediction quality not only in relative terms but also in absolute monetary units (rubles).

## 4 Experiment

In this section, we present the results of computational experiments for two predictive models (single- and dual-modalities). The predictive model was developed using the Transformers framework from the Hugging Face platform. All computations were performed on an NVIDIA V100 GPU.

#### 4.1 The Single-Modality Approach Performance

The results of the experiment on predicting return vectors using only time series data for classical and deep learning models are presented in a Table [12.](#page-12-0) A Table [9](#page-9-0) provides the averaged prediction quality metrics for all models, sorted in ascending order of the mean absolute percentage error (MAPE) (column "Deviation").

From the experiment results, it is evident that the recurrent model LSTM achieves the best classification performance (predicting upward or downward trends) and regression accuracy (smallest deviation of the predicted price from the actual price). However, it lags slightly in terms of the mean absolute error metric.

| Model  | Accuracy, % | MAPE, % |
|--------|-------------|---------|
| LSTM   | 52.020      | 0.397   |
| XGB    | 45.000      | 1.627   |
| KNN    | 46.010      | 1.631   |
| RF     | 48.384      | 1.646   |
| LinReg | 50.152      | 1.669   |
| DT     | 49.798      | 1.824   |

<span id="page-9-0"></span>Table 9: The Single-Modality approach forecast (time-series) inference metrics: Accuracy and MAPE in percentage.

#### 4.2 The Dual-Modality Approach Performance

The results of the second experiment, which involved merging the news stream with numerical time series data and comparing the proposed multimodal approach with a forecast based solely on candlestick time series, are presented in the Table [13.](#page-13-0)

The Table [10](#page-9-1) provides the averaged prediction quality metrics for the considered models. The data in this table is sorted by the "Deviation" column in ascending order, reflecting the mean absolute percentage error (MAPE) of the predicted price deviations.

In this second experiment, the LSTM neural network was chosen as the baseline model. We compared different vectorization methods (RuBert, Qwen) and aggregation techniques (Sum, Mean) to evaluate their impact on prediction performance.

| Model            | Accuracy, % | MAPE, % |
|------------------|-------------|---------|
| LSTM-Qwen-Mean   | 48.552      | 0.256   |
| LSTM-Qwen-Sum    | 46.970      | 0.367   |
| LSTM             | 52.020      | 0.397   |
| LSTM-RuBert-Mean | 49.798      | 0.437   |
| LSTM-RuBert-Sum  | 48.148      | 0.445   |

<span id="page-9-1"></span>Table 10: The Dual-Modality Approach forecast. Accuracy, MAPE in percentage.

Figure [5](#page-10-0) shows the dependence of the mean squared error (MSE Loss) function values on the number of training iterations for different models, based on the training set (from July 7, 2022 to March 27, 2024) and the test set (from March 28 to August 30, 2024). The graph indicates that after 30 training epochs, the curves reach a stationary value.

The results from the tables suggest that the forecast based on the vectorized news stream using a large language model outperforms the forecast built solely on candlestick data of assets, demonstrating the smallest deviation of the pointwise price prediction from the actual price vector. Additionally, averaging the vectors (Mean) provides the best results.

The dataset (176 stocks of Russian companies traded on the Moscow Exchange and 79,555 Russian-language financial news articles) collected for the study is available at [\[11\]](#page-14-9).

![](_page_10_Figure_0.jpeg)

<span id="page-10-0"></span>Figure 5: Dependence of the mean squared error function values on the number of training iterations for different models. Training and test sets.

## 5 Discussion and Conclusion

As a result of the conducted experiments, we demonstrated that adding a textual modality —analyzing the news stream — positively impacts the accuracy of price prediction. On average, the MAPE metric (the deviation of the predicted price from the actual price) decreases by 55%: from 0.397 (LSTM model) to 0.256 (LSTM-Qwen-Mean model). Additionally, predictions based on vectors obtained using the large language model Vikhr-Qwen2.5-0.5b-Instruct outperformed those based on RuBert. This can be partly attributed to the fact that the Qwen model has a significantly larger context window and is trained on a larger text corpus with support for "Chain-of-Thought" (CoT) reasoning. This enhances the model's ability to reason and capture complex semantic dependencies within the text. The experimental results indicate that the averaging method (Mean) performed better than summation (Sum) and is the preferred method for aggregating news stream vectors.

At the same time, it is important to note that the test data, on which the final metric values were calculated, covers the period from March 28 to August 30, 2024. During this period, the Russian securities market exhibited a general downward trend. The presence of a clear trend is a significant factor that simplifies the prediction task. However, even in this setting, the proposed multimodal approach proved to be the best among those considered.

The training and validation of the model for the rewriting task were conducted on news articles whose length did not exceed the context window of the RuBert model. As a result, artifacts related to the context window size only became apparent during the forecasting phase when the news dataset included articles averaging around 290 words in length. For future

| Model          | Ticker | R2    | MAPE, % | MAE   |
|----------------|--------|-------|---------|-------|
| LSTM-Qwen-Mean | AAPL   | 0.989 | 0.628   | 0.003 |
| Baseline       | AAPL   | 0.947 | 2.333   | 0.018 |
| LSTM-Qwen-Mean | AMZN   | 0.968 | 1.601   | 0.013 |
| Baseline       | AMZN   | 0.870 | 1.730   | 0.015 |
| LSTM-Qwen-Mean | GOOGL  | 0.935 | 1.394   | 0.008 |
| Baseline       | GOOGL  | 0.788 | 2.286   | 0.020 |
| LSTM-Qwen-Mean | NFLX   | 0.955 | 2.361   | 0.076 |
| Baseline       | NFLX   | 0.919 | 2.512   | 0.019 |
| LSTM-Qwen-Mean | TSLA   | 0.915 | 3.206   | 0.006 |
| Baseline       | TSLA   | 0.930 | 7.423   | 0.034 |

<span id="page-11-0"></span>Table 11: Multimodal approach forecasting metrics in comparison with the approach based on news sentiment score (Baseline) offered by [\[7\]](#page-14-6)

improvements in news filtering and classification by company, it is necessary to utilize models with a larger context window, such as Qwen.

The collected dataset [\[11\]](#page-14-9) demonstrates good structuring and can be used for fine-tuning large language models in Russian or adapted for the Russian language for applications in the financial sector.

For a quantitative comparison of the proposed model, we conducted a computational experiment based on the approach and metrics from the study [\[7\]](#page-14-6). Following the methodology of [\[7\]](#page-14-6), we used time series data of stock prices from five major American companies: AAPL, AMZN, GOOGL, NFLX, and TSLA, along with a dataset of English-language news articles labeled by company for the period from October 12, 2012 to January 31, 2020 (Table [11\)](#page-11-0).

It is worth noting that the dataset used includes text data in English; therefore, we utilized the original Qwen2.5-0.5b-Instruct model [\[10\]](#page-14-10) for news vectorization. To generate forecasts, we selected and trained the LSTM-Qwen-Mean model, as it demonstrated the best overall performance in our study. For evaluation, we used the coefficient of determination (R2 ), mean absolute error (MAE), and mean absolute percentage error (MAPE).

Thus, we worked with the same time series and evaluation metrics. Across all metrics, except for MAE on NFLX and R2 on TSLA, the proposed multimodal approach with vector averaging outperformed the best-performing results from the approach in [\[7\]](#page-14-6). Based on our computational experiments, we conclude that the proposed multimodal approach demonstrated superior forecasting quality and greater adaptability to both Russian and international markets.

In the future, it is necessary to explore how to incorporate the incoming news stream into the predictive model—specifically, the optimal time window for using news data and the best approach for weighting news messages (e. g., adjusting the weight of a news article based on its chronological position in the dataset).

### Acknowledgments

This work was supported by the grant of the state program of the "Sirius" Federal Territory "Scientific and technological development of the "Sirius" Federal Territory" (Agreement No. 18-03 date 10.09.2024).

<span id="page-12-0"></span>

| Mo<br>del      | Me<br>tal<br>and<br>Mi<br>nin<br>s<br>g |                | Oil<br>and<br>Ga<br>s |                | Co<br>Sec<br>tor<br>nsu<br>me<br>r |                | Co<br>tio<br>nst<br>ruc<br>n |                | lec<br>Te<br>nic<br>ati<br>om<br>mu<br>ons |                | Tra<br>ort<br>nsp |                | Fin<br>anc<br>e |                | Ch<br>l<br>Ind<br>ica<br>ust<br>em<br>ry |                | Po<br>En<br>ine<br>eri<br>we<br>r<br>g<br>ng |                |
|----------------|-----------------------------------------|----------------|-----------------------|----------------|------------------------------------|----------------|------------------------------|----------------|--------------------------------------------|----------------|-------------------|----------------|-----------------|----------------|------------------------------------------|----------------|----------------------------------------------|----------------|
|                | MT<br>LR                                | TR<br>MK       | SN<br>GS              | SIB<br>N       | MG<br>NT                           | LE<br>NT       | PIK<br>K                     | SM<br>LT       | MT<br>SS                                   | RT<br>KM<br>P  | AF<br>LT          | FL<br>OT       | BS<br>PB        | SF<br>IN       | PH<br>OR                                 | KZ<br>OS<br>P  | HY<br>DR                                     | MR<br>KC       |
| LS<br>TM       | 56<br>.36<br>4                          | 56<br>.36<br>4 | 50.<br>303            | 58<br>.18<br>2 | 46.<br>667                         | 56<br>.36<br>4 | 49.<br>091                   | 53<br>.93<br>9 | .97<br>56<br>0                             | 55<br>.15<br>2 | 55.<br>152        | 47.<br>273     | 46.<br>061      | 7<br>49<br>.69 | 41.<br>818                               | 57<br>.57<br>6 | 59.<br>394                                   | 40.<br>000     |
|                | 0.4<br>10                               | 0.3<br>62      | 0.3<br>52             | 0.3<br>41      | 0.3<br>31                          | 71<br>0.3      | 0.4<br>84                    | 0.3<br>28      | 0.5<br>41                                  | 0.2<br>46      | 0.4<br>19         | 0.2<br>58      | 0.4<br>10       | 47<br>0.4      | 0.2<br>31                                | 0.4<br>58      | 0.3<br>80                                    | 0.7<br>68      |
| XG<br>B        | 40.<br>000                              | 40.<br>909     | 49.<br>091            | 40.<br>000     | 39.<br>091                         | 54.<br>546     | 40.<br>909                   | 42.<br>727     | 42.<br>727                                 | 45.<br>455     | 46.<br>364        | 43.<br>637     | 49.<br>091      | 40.<br>000     | 42.<br>727                               | 49.<br>091     | 51.<br>182                                   | 51.<br>182     |
|                | 2.0<br>89                               | 2.1<br>05      | 1.7<br>76             | 1.7<br>66      | 1.5<br>17                          | 2.2<br>02      | 1.5<br>65                    | 1.5<br>77      | 1.2<br>90                                  | 1.2<br>99      | 2.0<br>79         | 2.1<br>16      | 1.6<br>12       | 1.6<br>03      | 1.1<br>94                                | 1.1<br>98      | 1.1<br>24                                    | 1.1<br>82      |
| KN<br>N        | 42.<br>273                              | 38.<br>182     | 48.<br>182            | 58<br>.18<br>2 | 43.<br>636                         | 39.<br>091     | 50.<br>909                   | 38.<br>182     | 40.<br>000                                 | 42.<br>723     | 57<br>.27<br>3    | 38.<br>182     | 50.<br>909      | 30.<br>909     | .72<br>52<br>3                           | 42.<br>723     | 60<br>.00<br>0                               | 49.<br>091     |
|                | 2.0<br>50                               | 2.1<br>67      | 1.7<br>75             | 46<br>1.7      | 1.4<br>93                          | 2.1<br>78      | 1.5<br>63                    | 1.5<br>52      | 1.3<br>06                                  | 1.3<br>03      | 2.0<br>17         | 24<br>2.1      | 1.6<br>95       | 47<br>1.6      | 49<br>1.1                                | 1.2<br>37      | 1.1<br>30                                    | 1.2<br>25      |
| RF             | 50.<br>909                              | 47.<br>273     | 50.<br>000            | 46.<br>364     | 49.<br>091                         | 52.<br>723     | 50.<br>000                   | 46.<br>364     | 45.<br>455                                 | 42.<br>727     | 52.<br>727        | 42.<br>727     | 50.<br>909      | 39.<br>091     | 48.<br>182                               | 49.<br>091     | 48.<br>182                                   | 54.<br>545     |
|                | 2.0<br>20                               | 2.1<br>54      | 1.7<br>35             | 1.7<br>88      | 1.5<br>19                          | 2.1<br>45      | 1.5<br>58                    | 1.5<br>39      | 1.5<br>20                                  | 1.3<br>35      | 2.0<br>62         | 2.1<br>04      | 1.5<br>98       | 1.7<br>43      | 1.1<br>68                                | 1.2<br>10      | 1.2<br>14                                    | 1.2<br>14      |
| Lin<br>Re<br>g | 50.<br>000                              | 49.<br>091     | 60<br>.90<br>9        | 41.<br>818     | 40.<br>000                         | 51.<br>818     | 44.<br>545                   | 49.<br>091     | 53.<br>636                                 | 50.<br>909     | 60.<br>909        | 45.<br>454     | 54<br>.54<br>5  | 48.<br>182     | 50.<br>000                               | 46.<br>364     | 45.<br>455                                   | 50.<br>000     |
|                | 2.0<br>29                               | 2.1<br>14      | 1.7<br>44             | 1.8<br>39      | 1.7<br>09                          | 2.2<br>20      | 1.6<br>37                    | 1.5<br>36      | 1.4<br>19                                  | 1.3<br>55      | 1.9<br>76         | 2.0<br>74      | 1.6<br>02       | 1.9<br>60      | 1.2<br>27                                | 1.2<br>17      | 1.1<br>51                                    | 1.2<br>24      |
| DT             | 42.<br>727                              | 727<br>52.     | 727<br>52.            | 51.<br>818     | 60<br>.00<br>0                     | 51.<br>818     | 51<br>.81<br>8               | 41.<br>818     | 50.<br>000                                 | 48.<br>182     | 51.<br>818        | 49<br>.09<br>1 | 45.<br>455      | 41.<br>818     | 45.<br>455                               | 54.<br>545     | 49.<br>091                                   | 55<br>.45<br>5 |
|                | 79<br>2.6                               | 2.3<br>08      | 57<br>1.8             | 1.8<br>13      | 72<br>1.6                          | 2.5<br>89      | 1.5<br>92                    | 1.6<br>83      | 1.3<br>95                                  | 1.4<br>11      | 94<br>2.1         | 94<br>2.2      | 1.8<br>29       | 1.9<br>59      | 1.2<br>18                                | 1.5<br>81      | 1.3<br>55                                    | 1.4<br>03      |

Table 12: Returns vector forecasting metrics with only time-series in use. Accuracy (the upper row), MAPE (the lower row) in percentage

<span id="page-13-0"></span>Table 13: The Dual-Modality returns vector forecasting metrics. Accuracy (the upper row), MAPE (the lower row) in percentage.

| Mo<br>del                          | Me<br>tals<br>and<br>Min<br>ing |                 | Oil<br>and<br>Gas |                 | Con<br>Sec<br>tor<br>sum<br>er |                 | Con<br>stru<br>ctio<br>n |                 | Tel<br>uni<br>cat<br>ion<br>eco<br>mm<br>s |                 | Tra<br>ort<br>nsp |                 | Fin<br>anc<br>e |                 | Che<br>al<br>Ind<br>mic<br>ust<br>ry |                 | Pow<br>Eng<br>inee<br>ring<br>er |                 |
|------------------------------------|---------------------------------|-----------------|-------------------|-----------------|--------------------------------|-----------------|--------------------------|-----------------|--------------------------------------------|-----------------|-------------------|-----------------|-----------------|-----------------|--------------------------------------|-----------------|----------------------------------|-----------------|
|                                    | MT<br>LR                        | TR<br>MK        | SNG<br>S          | SIB<br>N        | MG<br>NT                       | LEN<br>T        | PIK<br>K                 | SM<br>LT        | MT<br>SS                                   | RT<br>KM<br>P   | AF<br>LT          | FLO<br>T        | BSP<br>B        | SFI<br>N        | PH<br>OR                             | KZ<br>OSP       | HY<br>DR                         | MR<br>KC        |
| illa<br>LST<br>M<br>van            | %<br>56.<br>364                 | %<br>56.<br>364 | 03%<br>50.3       | %<br>58.<br>182 | 67%<br>46.6                    | %<br>56.<br>364 | 91%<br>49.0              | %<br>53.<br>939 | %<br>56.<br>970                            | %<br>55.<br>152 | %<br>55.<br>152   | 73%<br>47.2     | 61%<br>46.0     | 97%<br>49.6     | 18%<br>41.8                          | %<br>57.<br>576 | 94%<br>59.3                      | 00%<br>40.0     |
|                                    | 10%<br>0.4                      | 2%<br>0.36      | 2%<br>0.35        | 1%<br>0.34      | 1%<br>0.33                     | 1%<br>0.37      | 4%<br>0.48               | 8%<br>0.32      | 1%<br>0.54                                 | 6%<br>0.24      | 19%<br>0.4        | 58%<br>0.2      | 10%<br>0.4      | 7%<br>0.44      | 1%<br>0.23                           | 8%<br>0.45      | 0%<br>0.38                       | 8%<br>0.76      |
| LST<br>M<br>RuB<br>SUM<br>ert      | 94%<br>39.3                     | %<br>35.<br>152 | 39%<br>53.9       | %<br>58.<br>182 | %<br>53.<br>333                | 91%<br>49.0     | 03%<br>50.3              | 88%<br>38.7     | 39%<br>53.9                                | 97%<br>49.6     | 15%<br>51.5       | 36%<br>43.6     | 79%<br>47.8     | 48%<br>44.8     | 33%<br>53.3                          | 24%<br>42.4     | 88%<br>58.7                      | 24%<br>42.4     |
|                                    | 9%<br>0.40                      | 2%<br>0.39      | 5%<br>0.86        | 5%<br>0.26      | 17%<br>0.4                     | 0%<br>0.40      | 2%<br>0.46               | 00%<br>0.2      | 3%<br>0.47                                 | 4%<br>0.27      | 1%<br>0.64        | 2%<br>0.53      | 6%<br>0.40      | 5%<br>0.44      | 4%<br>0.26                           | 2%<br>0.49      | 6%<br>0.32                       | 2%<br>0.74      |
| LST<br>AN<br>M<br>RuB<br>ME<br>ert | 88%<br>38.7                     | 24%<br>42.4     | %<br>58.<br>182   | %<br>58.<br>182 | 79%<br>47.8                    | 09%<br>50.9     | %<br>57.<br>576          | 61%<br>46.0     | 52%<br>55.1                                | 55%<br>45.4     | 03%<br>50.3       | %<br>52.<br>121 | 09%<br>50.9     | 73%<br>47.2     | 52%<br>55.1                          | 12%<br>41.2     | 58%<br>55.7                      | %<br>43.<br>030 |
|                                    | 10%<br>0.4                      | 92%<br>0.1      | 4%<br>1.82        | 16%<br>0.2      | 9%<br>0.29                     | 9%<br>0.35      | 6%<br>0.43               | 0%<br>0.27      | 8%<br>0.36                                 | 1%<br>0.27      | 8%<br>0.34        | 2%<br>0.26      | 6%<br>0.32      | 0%<br>0.39      | 8%<br>0.23                           | 1%<br>0.49      | 1%<br>0.32                       | 9%<br>0.83      |
| LST<br>M<br>QW<br>EN<br>SUM        | 55%<br>45.4                     | 64%<br>36.3     | 48%<br>44.8       | 94%<br>39.3     | 61%<br>46.0                    | 33%<br>53.3     | 73%<br>47.2              | 64%<br>36.3     | 79%<br>47.8                                | 48%<br>44.8     | 55%<br>45.4       | 36%<br>43.6     | 79%<br>47.8     | %<br>56.<br>970 | %<br>60.<br>000                      | 85%<br>48.4     | 79%<br>47.8                      | 24%<br>42.4     |
|                                    | 2%<br>0.52                      | 4%<br>0.50      | 7%<br>0.30        | 8%<br>0.36      | 7%<br>0.30                     | 6%<br>0.34      | 9%<br>0.52               | 11%<br>0.3      | 16%<br>0.3                                 | 71%<br>0.1      | 9%<br>0.25        | 2%<br>0.39      | 9%<br>0.36      | 95%<br>0.1      | 4%<br>0.35                           | 9%<br>0.36      | 2%<br>0.29                       | 0%<br>0.66      |
| LST<br>M<br>QW<br>EN<br>ME<br>AN   | 21%<br>52.1                     | 58%<br>35.7     | 97%<br>49.6       | 79%<br>47.8     | 85%<br>48.4                    | 21%<br>52.1     | 33%<br>53.3              | 30%<br>43.0     | 55%<br>45.4                                | 42%<br>44.2     | 21%<br>52.1       | 36%<br>43.6     | %<br>52.<br>121 | %<br>56.<br>970 | 48%<br>44.8                          | 97%<br>49.6     | %<br>61.<br>212                  | 18%<br>41.8     |
|                                    | 46%<br>0.2                      | 19%<br>0.4      | 06%<br>0.1        | 65%<br>0.1      | 35%<br>0.2                     | 31%<br>0.3      | 22%<br>0.3               | 1%<br>0.24      | 93%<br>0.1                                 | 8%<br>0.17      | 82%<br>0.1        | 5%<br>0.34      | 27%<br>0.2      | 2%<br>0.27      | 19%<br>0.2                           | 52%<br>0.3      | 78%<br>0.1                       | 43%<br>0.5      |

### References

- <span id="page-14-1"></span>[1] K. Mishev and A. Gjorgjevikj and I. Vodenska and L. Chitkushev and D. Trajanov, Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers, IEEE Access, vol. 8, 2020, 131662–131682.
- <span id="page-14-2"></span>[2] H. Trang-Thi and H. Yennun, Stock Price Movement Prediction Using Sentiment Analysis and CandleStick Chart Representation, Sensors, vol. 21, No. 23, 2021, 7957.
- <span id="page-14-0"></span>[3] M. Jaggi and P. Mandal and S. Narang and U. Naseem and M. Khushi, Text Mining of Stocktwits Data for Predicting Stock Prices, Applied System Innovation, vol. 4, No. 1, 2021, 13.
- <span id="page-14-3"></span>[4] , B. Fazlija and P. Harder, Using Financial News Sentiment for Stock Price Direction Prediction, Mathematics, vol. 10, No. 13, 2022.
- <span id="page-14-4"></span>[5] Xinli, Yu and Zheng, Chen and Yuan, Ling and Shujing, Dong and Zongyi Liu and Yanbin Lu, Temporal Data Meets LLM – Explainable Financial Time Series Forecasting,[https:](https://arxiv.org/abs/2306.11025) [//arxiv.org/abs/2306.11025](https://arxiv.org/abs/2306.11025), 2023, arXiv preprint.
- <span id="page-14-5"></span>[6] Boyu, Zhang and Hongyang, Yang and Xiao-Yang, Liu, Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models, <https://arxiv.org/abs/2306.12659>, 2023, arXiv preprint.
- <span id="page-14-6"></span>[7] T.D. Kulikova and E.Y. Kovtun and S.A. Budennyy, Do We Benefit from the Categorization of the News Flow in the Stock Price Prediction Problem?, Dokl. Math., vol. 108, No. Suppl 2, 2023, S503–S510, 10.1134/S1064562423701648
- <span id="page-14-7"></span>[8] Y. Kuratov and M. Arkhipov, Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language, <https://arxiv.org/abs/1905.07213>, 2019, arXiv preprint.
- <span id="page-14-8"></span>[9] A. Nikolich and K. Korolev and S. Bratchikov and N. Kompanets and A. Shelmanov, Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian, <https://arxiv.org/pdf/2405.13929>, 2024, arXiv preprint.
- <span id="page-14-10"></span>[10] Jinze, Bai and Shuai, Bai and Yunfei, Chu and Zeyu, Cui and other, Qwen2 Technical Report, <https://arxiv.org/abs/2309.16609>, 2023, arXiv preprint.
- <span id="page-14-9"></span>[11] K. Khubiev, Russian Financial News Dataset, [https://www.kaggle.com/datasets/](https://www.kaggle.com/datasets/kkhubiev/russian-financial-news) [kkhubiev/russian-financial-news](https://www.kaggle.com/datasets/kkhubiev/russian-financial-news), 2025, Kaggle Platform.