<p align="center"><h1 align="center">üåü QuantMind üåü</h1><h2 align="center">Automatically Quantitative Finance Papers List</h2></p>
<p align="center"><img src="https://raw.githubusercontent.com/LLMQuant/quant-mind/main/asset/icon.png" width="180"></p>

## üö© Updated on 2025.06.17

<details>
  <summary><strong>üìú Contents</strong></summary>
  <ol>
    <li><a href=#-machine-learning-in-finance>üìå Machine Learning in Finance</a></li>
    <li><a href=#-deep-learning-in-finance>üìå Deep Learning in Finance</a></li>
    <li><a href=#-reinforcement-learning-in-finance>üìå Reinforcement Learning in Finance</a></li>
    <li><a href=#-time-series-forecasting>üìå Time Series Forecasting</a></li>
  </ol>
</details>

## üìå Machine Learning in Finance

| üìÖ Publish Date | üìñ Title | üë®‚Äçüíª Authors | üîó PDF | üíª Code | üí¨ Comment | üìú Abstract |
|:--------------:|:----------------------------|:------------------|:------:|:------:|:-------:|:--------|
| 2025-06-11 | Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets | Md. Yeasin Rahat, Rajan Das Gupta, Nur Raisa Rahman et.al. | [2506.09851](http://arxiv.org/abs/2506.09851) |  | Accepted in MECON 2025 | <details><summary>Abstract (click to expand)</summary>The prediction of foreign exchange rates, such as the US Dollar (USD) to Bangladeshi Taka (BDT), plays a pivotal role in global financial markets, influencing trade, investments, and economic stability. This study leverages historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo Finance, to develop advanced machine learning models for accurate forecasting. A Long Short-Term Memory (LSTM) neural network is employed, achieving an exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and a test loss of 0.8523, significantly outperforming traditional methods like ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is applied for directional prediction, with backtesting on a $10,000 initial capital revealing a 40.82% profitable trade rate, though resulting in a net loss of$ 20,653.25 over 49 trades. The study analyzes historical trends, showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates normalized daily returns to capture volatility. These findings highlight the potential of deep learning in forex forecasting, offering traders and policymakers robust tools to mitigate risks. Future work could integrate sentiment analysis and real-time economic indicators to further enhance model adaptability in volatile markets.</details> |
| 2025-06-10 | EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements | Issa Sugiura, Takashi Ishida, Taro Makino et.al. | [2506.08762](http://arxiv.org/abs/2506.08762) |  |  | <details><summary>Abstract (click to expand)</summary>Financial analysis presents complex challenges that could leverage large language model (LLM) capabilities. However, the scarcity of challenging financial datasets, particularly for Japanese financial data, impedes academic innovation in financial analytics. As LLMs advance, this lack of accessible research resources increasingly hinders their development and evaluation in this specialized domain. To address this gap, we introduce EDINET-Bench, an open-source Japanese financial benchmark designed to evaluate the performance of LLMs on challenging financial tasks including accounting fraud detection, earnings forecasting, and industry prediction. EDINET-Bench is constructed by downloading annual reports from the past 10 years from Japan's Electronic Disclosure for Investors' NETwork (EDINET) and automatically assigning labels corresponding to each evaluation task. Our experiments reveal that even state-of-the-art LLMs struggle, performing only slightly better than logistic regression in binary classification for fraud detection and earnings forecasting. These results highlight significant challenges in applying LLMs to real-world financial applications and underscore the need for domain-specific adaptation. Our dataset, benchmark construction code, and evaluation code is publicly available to facilitate future research in finance with LLMs.</details> |
| 2025-06-09 | Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting | Timoth√©e Hornek Amir Sartipi, Igor Tchappi, Gilbert Fridgen et.al. | [2506.08113](http://arxiv.org/abs/2506.08113) |  |  | <details><summary>Abstract (click to expand)</summary>Accurate electricity price forecasting (EPF) is crucial for effective decision-making in power trading on the spot market. While recent advances in generative artificial intelligence (GenAI) and pre-trained large language models (LLMs) have inspired the development of numerous time series foundation models (TSFMs) for time series forecasting, their effectiveness in EPF remains uncertain. To address this gap, we benchmark several state-of-the-art pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and TimeGPT--against established statistical and machine learning (ML) methods for EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany, France, the Netherlands, Austria, and Belgium, we generate daily forecasts with a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the TSFMs, performing on par with traditional models. However, the biseasonal MSTL model, which captures daily and weekly seasonality, stands out for its consistent performance across countries and evaluation metrics, with no TSFM statistically outperforming it.</details> |
| 2025-06-09 | Predicting Realized Variance Out of Sample: Can Anything Beat The Benchmark? | Austin Pollok et.al. | [2506.07928](http://arxiv.org/abs/2506.07928) |  |  | <details><summary>Abstract (click to expand)</summary>The discrepancy between realized volatility and the market's view of volatility has been known to predict individual equity options at the monthly horizon. It is not clear how this predictability depends on a forecast's ability to predict firm-level volatility. We consider this phenomenon at the daily frequency using high-dimensional machine learning models, as well as low-dimensional factor models. We find that marginal improvements to standard forecast error measurements can lead to economically significant gains in portfolio performance. This makes a case for re-imagining the way we train models that are used to construct portfolios.</details> |
| 2025-06-11 | The Subtle Interplay between Square-root Impact, Order Imbalance & Volatility: A Unifying Framework | Guillaume Maitrier, Jean-Philippe Bouchaud et.al. | [2506.07711](http://arxiv.org/abs/2506.07711) |  |  | <details><summary>Abstract (click to expand)</summary>In this work, we aim to reconcile several apparently contradictory observations in market microstructure: is the famous ''square-root law'' of metaorder impact that decays with time compatible with the random-walk nature of prices and the linear impact of order imbalances? Can one entirely explain the volatility of prices as resulting from the flow of uninformed metaorders that mechanically impact prices? We introduce a new theoretical framework to describe metaorders with different signs, sizes and durations, which all impact prices as a square-root of volume but with a subsequent time decay. We show that, as in the original propagator model, price diffusion is ensured by the long memory of cross-correlations between metaorders. In order to account for the effect of strongly fluctuating volumes $q$ of individual trades, we further introduce two $q$-dependent exponents, which allows us to account for the way the moments of generalized volume imbalance and the correlation between price changes and generalized order flow imbalance scales with $T$. We predict in particular that the corresponding power-laws depend in a non-monotonic fashion on a parameter $a$ that allows one to put the same weight on all child orders or overweight large orders, a behaviour clearly borne out by empirical data. We also predict that the correlation between price changes and volume imbalances should display a maximum as a function of $a$ , which again matches observations. Such noteworthy agreement between theory and data suggests that our framework correctly captures the basic mechanism at the heart of price formation, namely the average impact of metaorders. We argue that our results support the ''Order-Driven'' theory of excess volatility, and are at odds with the idea that a ''Fundamental'' component accounts for a large share of the volatility of financial markets.</details> |
| 2025-05-22 | Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation | Zonghan Wu, Junlin Wang, Congyuan Zou et.al. | [2506.07315](http://arxiv.org/abs/2506.07315) |  |  | <details><summary>Abstract (click to expand)</summary>Generative AI, particularly large language models (LLMs), is beginning to transform the financial industry by automating tasks and helping to make sense of complex financial information. One especially promising use case is the automatic creation of fundamental analysis reports, which are essential for making informed investment decisions, evaluating credit risks, guiding corporate mergers, etc. While LLMs attempt to generate these reports from a single prompt, the risks of inaccuracy are significant. Poor analysis can lead to misguided investments, regulatory issues, and loss of trust. Existing financial benchmarks mainly evaluate how well LLMs answer financial questions but do not reflect performance in real-world tasks like generating financial analysis reports. In this paper, we propose FinAR-Bench, a solid benchmark dataset focusing on financial statement analysis, a core competence of fundamental analysis. To make the evaluation more precise and reliable, we break this task into three measurable steps: extracting key information, calculating financial indicators, and applying logical reasoning. This structured approach allows us to objectively assess how well LLMs perform each step of the process. Our findings offer a clear understanding of LLMs current strengths and limitations in fundamental analysis and provide a more practical way to benchmark their performance in real-world financial settings.</details> |
| 2025-06-08 | Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling | Hans Buehler, Blanka Horvath, Yannick Limmer et.al. | [2506.07299](http://arxiv.org/abs/2506.07299) |  | 18 pages, 12 figures | <details><summary>Abstract (click to expand)</summary>This paper addresses the challenge of model uncertainty in quantitative finance, where decisions in portfolio allocation, derivative pricing, and risk management rely on estimating stochastic models from limited data. In practice, the unavailability of the true probability measure forces reliance on an empirical approximation, and even small misestimations can lead to significant deviations in decision quality. Building on the framework of Klibanoff et al. (2005), we enhance the conventional objective - whether this is expected utility in an investing context or a hedging metric - by superimposing an outer "uncertainty measure", motivated by traditional monetary risk measures, on the space of models. In scenarios where a natural model distribution is lacking or Bayesian methods are impractical, we propose an ad hoc subsampling strategy, analogous to bootstrapping in statistical finance and related to mini-batch sampling in deep learning, to approximate model uncertainty. To address the quadratic memory demands of naive implementations, we also present an adapted stochastic gradient descent algorithm that enables efficient parallelization. Through analytical, simulated, and empirical studies - including multi-period, real data and high-dimensional examples - we demonstrate that uncertainty measures outperform traditional mixture of measures strategies and our model-agnostic subsampling-based approach not only enhances robustness against model risk but also achieves performance comparable to more elaborate Bayesian methods.</details> |
| 2025-06-02 | An analysis of capital market through the lens of integral transforms: exploring efficient markets and information asymmetry | Kiran Sharma, Abhijit Dutta, Rupak Mukherjee et.al. | [2506.06350](http://arxiv.org/abs/2506.06350) |  |  | <details><summary>Abstract (click to expand)</summary>Post Modigliani and Miller (1958), the concept of usage of arbitrage created a permanent mark on the discourses of financial framework. The arbitrage process is largely based on information dissemination amongst the stakeholders operating in the financial market. The advent of the efficient market Hypothesis draws close to the M&M hypothesis. Giving importance to the arbitrage process, which effects the price discovery in the stock market. This divided the market as random and efficient cohort system. The focus was on which information forms a key factor in deciding the price formation in the market. However, the conventional techniques of analysis do not permit the price cycles to be interpreted beyond its singular wave-like cyclical movement. The apparent cyclic measurement is not coherent as the technical analysis does not give sustained result. Hence adaption of theories and computation from mathematical methods of physics ensures that these cycles are decomposed and the effect of the broken-down cycles is interpreted to understand the overall effect of information on price formation and discovery. In order to break the cycle this paper uses spectrum analysis to decompose and understand the above-said phenomenon in determining the price behavior in National Stock Exchange of India (NSE).</details> |
| 2025-06-01 | Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100 | Sukru Selim Calik, Andac Akyuz, Zeynep Hilal Kilimci et.al. | [2506.06345](http://arxiv.org/abs/2506.06345) |  |  | <details><summary>Abstract (click to expand)</summary>Financial literacy is increasingly dependent on the ability to interpret complex financial data and utilize advanced forecasting tools. In this context, this study proposes a novel approach that combines transformer-based time series models with explainable artificial intelligence (XAI) to enhance the interpretability and accuracy of stock price predictions. The analysis focuses on the daily stock prices of the five highest-volume banks listed in the BIST100 index, along with XBANK and XU100 indices, covering the period from January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla Transformer, and Time Series Transformer are employed, with input features enriched by technical indicators. SHAP and LIME techniques are used to provide transparency into the influence of individual features on model outputs. The results demonstrate the strong predictive capabilities of transformer models and highlight the potential of interpretable machine learning to empower individuals in making informed investment decisions and actively engaging in financial markets.</details> |
| 2025-05-30 | The Hype Index: an NLP-driven Measure of Market News Attention | Zheng Cao, Wanchaloem Wunkaew, Helyette Geman et.al. | [2506.06329](http://arxiv.org/abs/2506.06329) |  |  | <details><summary>Abstract (click to expand)</summary>This paper introduces the Hype Index as a novel metric to quantify media attention toward large-cap equities, leveraging advances in Natural Language Processing (NLP) for extracting predictive signals from financial news. Using the S&P 100 as the focus universe, we first construct a News Count-Based Hype Index, which measures relative media exposure by computing the share of news articles referencing each stock or sector. We then extend it to the Capitalization Adjusted Hype Index, adjusts for economic size by taking the ratio of a stock's or sector's media weight to its market capitalization weight within its industry or sector. We compute both versions of the Hype Index at the stock and sector levels, and evaluate them through multiple lenses: (1) their classification into different hype groups, (2) their associations with returns, volatility, and VIX index at various lags, (3) their signaling power for short-term market movements, and (4) their empirical properties including correlations, samplings, and trends. Our findings suggest that the Hype Index family provides a valuable set of tools for stock volatility analysis, market signaling, and NLP extensions in Finance.</details> |
| 2025-05-27 | A Sinusoidal Hull-White Model for Interest Rate Dynamics: Capturing Long-Term Periodicity in U.S. Treasury Yields | Amit Kumar Jha et.al. | [2506.06317](http://arxiv.org/abs/2506.06317) |  | 20 pages, 5 figures, 4 tables | <details><summary>Abstract (click to expand)</summary>This study is motivated by empirical observations of periodic fluctuations in interest rates, notably long-term economic cycles spanning decades, which the conventional Hull-White short-rate model fails to adequately capture. To address this limitation, we propose an extension that incorporates a sinusoidal, time-varying mean reversion speed, allowing the model to reflect cyclic interest rate dynamics more effectively.   The model is calibrated using a comprehensive dataset of daily U.S. Treasury yield curves obtained from the Federal Reserve Economic Data (FRED) database, covering the period from January 1990 to December 2022. The dataset includes tenors of 1, 2, 3, 5, 7, 10, 20, and 30 years, with the most recent yields ranging from 1.22% (1-year) to 2.36% (30-year).   Calibration is performed using the Nelder-Mead optimization algorithm, and Monte Carlo simulations with 200 paths and a time step of 0.05 years. The resulting 30-year zero-coupon bond price under the proposed model is 0.43, compared to 0.47 under the standard Hull-White model. This corresponds to root mean squared errors of 0.12% and 0.14%, respectively, indicating a noticeable improvement in fit, particularly for longer maturities.   These results highlight the model's enhanced capability to capture long-term yield dynamics and suggest significant implications for bond pricing, interest rate risk management, and the valuation of interest rate derivatives. The findings also open avenues for further research into stochastic periodicity and alternative interest rate modeling frameworks.</details> |
| 2025-05-22 | Enhancing Meme Token Market Transparency: A Multi-Dimensional Entity-Linked Address Analysis for Liquidity Risk Evaluation | Qiangqiang Liu, Qian Huang, Frank Fan et.al. | [2506.05359](http://arxiv.org/abs/2506.05359) |  | IEEE International Conference on Blockchain and Cryptocurrency (Proc.   IEEE ICBC 2025) | <details><summary>Abstract (click to expand)</summary>Meme tokens represent a distinctive asset class within the cryptocurrency ecosystem, characterized by high community engagement, significant market volatility, and heightened vulnerability to market manipulation. This paper introduces an innovative approach to assessing liquidity risk in meme token markets using entity-linked address identification techniques. We propose a multi-dimensional method integrating fund flow analysis, behavioral similarity, and anomalous transaction detection to identify related addresses. We develop a comprehensive set of liquidity risk indicators tailored for meme tokens, covering token distribution, trading activity, and liquidity metrics. Empirical analysis of tokens like BabyBonk, NMT, and BonkFork validates our approach, revealing significant disparities between apparent and actual liquidity in meme token markets. The findings of this study provide significant empirical evidence for market participants and regulatory authorities, laying a theoretical foundation for building a more transparent and robust meme token ecosystem.</details> |
| 2025-06-05 | Classification of Extremal Dependence in Financial Markets via Bootstrap Inference | Qian Hui, Sidney I. Resnick, Tiandong Wang et.al. | [2506.04656](http://arxiv.org/abs/2506.04656) |  |  | <details><summary>Abstract (click to expand)</summary>Accurately identifying the extremal dependence structure in multivariate heavy-tailed data is a fundamental yet challenging task, particularly in financial applications. Following a recently proposed bootstrap-based testing procedure, we apply the methodology to absolute log returns of U.S. S&P 500 and Chinese A-share stocks over a time period well before the U.S. election in 2024. The procedure reveals more isolated clustering of dependent assets in the U.S. economy compared with China which exhibits different characteristics and a more interconnected pattern of extremal dependence. Cross-market analysis identifies strong extremal linkages in sectors such as materials, consumer staples and consumer discretionary, highlighting the effectiveness of the testing procedure for large-scale empirical applications.</details> |
| 2025-06-09 | High-Dimensional Learning in Finance | Hasan Fallahgoul et.al. | [2506.03780](http://arxiv.org/abs/2506.03780) |  |  | <details><summary>Abstract (click to expand)</summary>Recent advances in machine learning have shown promising results for financial prediction using large, over-parameterized models. This paper provides theoretical foundations and empirical validation for understanding when and how these methods achieve predictive success. I examine two key aspects of high-dimensional learning in finance. First, I prove that within-sample standardization in Random Fourier Features implementations fundamentally alters the underlying Gaussian kernel approximation, replacing shift-invariant kernels with training-set dependent alternatives. Second, I establish information-theoretic lower bounds that identify when reliable learning is impossible no matter how sophisticated the estimator. A detailed quantitative calibration of the polynomial lower bound shows that with typical parameter choices, e.g., 12,000 features, 12 monthly observations, and R-square 2-3%, the required sample size to escape the bound exceeds 25-30 years of data--well beyond any rolling-window actually used. Thus, observed out-of-sample success must originate from lower-complexity artefacts rather than from the intended high-dimensional mechanism.</details> |
| 2025-05-18 | Why Regression? Binary Encoding Classification Brings Confidence to Stock Market Index Price Prediction | Junzhe Jiang, Chang Yang, Xinrun Wang et.al. | [2506.03153](http://arxiv.org/abs/2506.03153) |  |  | <details><summary>Abstract (click to expand)</summary>Stock market indices serve as fundamental market measurement that quantify systematic market dynamics. However, accurate index price prediction remains challenging, primarily because existing approaches treat indices as isolated time series and frame the prediction as a simple regression task. These methods fail to capture indices' inherent nature as aggregations of constituent stocks with complex, time-varying interdependencies. To address these limitations, we propose Cubic, a novel end-to-end framework that explicitly models the adaptive fusion of constituent stocks for index price prediction. Our main contributions are threefold. i) Fusion in the latent space: we introduce the fusion mechanism over the latent embedding of the stocks to extract the information from the vast number of stocks. ii) Binary encoding classification: since regression tasks are challenging due to continuous value estimation, we reformulate the regression into the classification task, where the target value is converted to binary and we optimize the prediction of the value of each digit with cross-entropy loss. iii) Confidence-guided prediction and trading: we introduce the regularization loss to address market prediction uncertainty for the index prediction and design the rule-based trading policies based on the confidence. Extensive experiments across multiple stock markets and indices demonstrate that Cubic consistently outperforms state-of-the-art baselines in stock index prediction tasks, achieving superior performance on both forecasting accuracy metrics and downstream trading profitability.</details> |
| 2025-05-14 | Beyond the Black Box: Interpretability of LLMs in Finance | Hariom Tatsat, Ariye Shater et.al. | [2505.24650](http://arxiv.org/abs/2505.24650) |  | 28 pages, 15 figures | <details><summary>Abstract (click to expand)</summary>Large Language Models (LLMs) exhibit remarkable capabilities across a spectrum of tasks in financial services, including report generation, chatbots, sentiment analysis, regulatory compliance, investment advisory, financial knowledge retrieval, and summarization. However, their intrinsic complexity and lack of transparency pose significant challenges, especially in the highly regulated financial sector, where interpretability, fairness, and accountability are critical. As far as we are aware, this paper presents the first application in the finance domain of understanding and utilizing the inner workings of LLMs through mechanistic interpretability, addressing the pressing need for transparency and control in AI systems. Mechanistic interpretability is the most intuitive and transparent way to understand LLM behavior by reverse-engineering their internal workings. By dissecting the activations and circuits within these models, it provides insights into how specific features or components influence predictions - making it possible not only to observe but also to modify model behavior. In this paper, we explore the theoretical aspects of mechanistic interpretability and demonstrate its practical relevance through a range of financial use cases and experiments, including applications in trading strategies, sentiment analysis, bias, and hallucination detection. While not yet widely adopted, mechanistic interpretability is expected to become increasingly vital as adoption of LLMs increases. Advanced interpretability tools can ensure AI systems remain ethical, transparent, and aligned with evolving financial regulations. In this paper, we have put special emphasis on how these techniques can help unlock interpretability requirements for regulatory and compliance purposes - addressing both current needs and anticipating future expectations from financial regulators globally.</details> |
| 2025-05-29 | Critical Dynamics of Random Surfaces and Multifractal Scaling | Christof Schmidhuber et.al. | [2505.23928](http://arxiv.org/abs/2505.23928) |  | 19 pages, 1 figure. Follow-up paper to arXiv:2409.05547 [hep-th] | <details><summary>Abstract (click to expand)</summary>The critical dynamics of conformal field theories on random surfaces is investigated beyond the dynamics of the overall area and the genus. It is found that the evolution of the order parameter in physical time is a multifractal random walk. Accordingly, the higher moments of time variations of the order parameter exhibit multifractal scaling. The series of Hurst exponents is computed and illustrated with the examples of the Ising-, 3-state-Potts-, and general minimal models on a random surface. Models are identified that can replicate the observed multifractal scaling in financial markets.</details> |
| 2025-05-28 | Model-Free Deep Hedging with Transaction Costs and Light Data Requirements | Pierre Brugi√®re, Gabriel Turinici et.al. | [2505.22836](http://arxiv.org/abs/2505.22836) |  |  | <details><summary>Abstract (click to expand)</summary>Option pricing theory, such as the Black and Scholes (1973) model, provides an explicit solution to construct a strategy that perfectly hedges an option in a continuous-time setting. In practice, however, trading occurs in discrete time and often involves transaction costs, making the direct application of continuous-time solutions potentially suboptimal. Previous studies, such as those by Buehler et al. (2018), Buehler et al. (2019) and Cao et al. (2019), have shown that deep learning or reinforcement learning can be used to derive better hedging strategies than those based on continuous-time models. However, these approaches typically rely on a large number of trajectories (of the order of $10^5$ or $10^6$ ) to train the model. In this work, we show that using as few as 256 trajectories is sufficient to train a neural network that significantly outperforms, in the Geometric Brownian Motion framework, both the classical Black & Scholes formula and the Leland model, which is arguably one of the most effective explicit alternatives for incorporating transaction costs. The ability to train neural networks with such a small number of trajectories suggests the potential for more practical and simple implementation on real-time financial series.</details> |
| 2025-05-27 | Replication of Reference-Dependent Preferences and the Risk-Return Trade-Off in the Chinese Market | Penggan Xu et.al. | [2505.20608](http://arxiv.org/abs/2505.20608) |  |  | <details><summary>Abstract (click to expand)</summary>This study replicates the findings of Wang et al. (2017) on reference-dependent preferences and their impact on the risk-return trade-off in the Chinese stock market, a unique context characterized by high retail investor participation, speculative trading behavior, and regulatory complexities. Capital Gains Overhang (CGO), a proxy for unrealized gains or losses, is employed to explore how behavioral biases shape cross-sectional stock returns in an emerging market setting. Utilizing data from 1995 to 2024 and econometric techniques such as Dependent Double Sorting and Fama-MacBeth regressions, this research investigates the interaction between CGO and five risk proxies: Beta, Return Volatility (RETVOL), Idiosyncratic Volatility (IVOL), Firm Age (AGE), and Cash Flow Volatility (CFVOL). Key findings reveal a weaker or absent positive risk-return relationship among high-CGO firms and stronger positive relationships among low-CGO firms, diverging from U.S. market results, and the interaction effects between CGO and risk proxies, significant and positive in the U.S., are predominantly negative in the Chinese market, reflecting structural and behavioral differences, such as speculative trading and diminished reliance on reference points. The results suggest that reference-dependent preferences play a less pronounced role in the Chinese market, emphasizing the need for tailored investment strategies in emerging economies.</details> |
| 2025-05-25 | Comparative analysis of financial data differentiation techniques using LSTM neural network | Dominik Stempie≈Ñ, Janusz Gajda et.al. | [2505.19243](http://arxiv.org/abs/2505.19243) |  | 71 pages, 21 figures, 14 tables | <details><summary>Abstract (click to expand)</summary>We compare traditional approach of computing logarithmic returns with the fractional differencing method and its tempered extension as methods of data preparation before their usage in advanced machine learning models. Differencing parameters are estimated using multiple techniques. The empirical investigation is conducted on data from four major stock indices covering the most recent 10-year period. The set of explanatory variables is additionally extended with technical indicators. The effectiveness of the differencing methods is evaluated using both forecast error metrics and risk-adjusted return trading performance metrics. The findings suggest that fractional differentiation methods provide a suitable data transformation technique, improving the predictive model forecasting performance. Furthermore, the generated predictions appeared to be effective in constructing profitable trading strategies for both individual assets and a portfolio of stock indices. These results underline the importance of appropriate data transformation techniques in financial time series forecasting, supporting the application of memory-preserving techniques.</details> |
| 2025-05-21 | Quantile Predictions for Equity Premium using Penalized Quantile Regression with Consistent Variable Selection across Multiple Quantiles | Shaobo Li, Ben Sherwood et.al. | [2505.16019](http://arxiv.org/abs/2505.16019) |  |  | <details><summary>Abstract (click to expand)</summary>This paper considers equity premium prediction, for which mean regression can be problematic due to heteroscedasticity and heavy-tails of the error. We show advantages of quantile predictions using a novel penalized quantile regression that offers a model for a full spectrum analysis on the equity premium distribution. To enhance model interpretability and address the well-known issue of crossing quantile predictions in quantile regression, we propose a model that enforces the selection of a common set of variables across all quantiles. Such a selection consistency is achieved by simultaneously estimating all quantiles with a group penalty that ensures sparsity pattern is the same for all quantiles. Consistency results are provided that allow the number of predictors to increase with the sample size. A Huberized quantile loss function and an augmented data approach are implemented for computational efficiency. Simulation studies show the effectiveness of the proposed approach. Empirical results show that the proposed method outperforms several benchmark methods. Moreover, we find some important predictors reverse their relationship to the excess return from lower to upper quantiles, potentially offering interesting insights to the domain experts. Our proposed method can be applied to other fields.</details> |
| 2025-05-20 | Cryptocurrencies in the Balance Sheet: Insights from (Micro)Strategy -- Bitcoin Interactions | Sabrina Aufiero, Antonio Briola, Tesfaye Salarin et.al. | [2505.14655](http://arxiv.org/abs/2505.14655) | **[link](https://github.com/financialcomputingucl/crypto_balance_sheets)** | 25 pages, 6 tables, 7 figures | <details><summary>Abstract (click to expand)</summary>This paper investigates the evolving link between cryptocurrency and equity markets in the context of the recent wave of corporate Bitcoin (BTC) treasury strategies. We assemble a dataset of 39 publicly listed firms holding BTC, from their first acquisition through April 2025. Using daily logarithmic returns, we first document significant positive co-movements via Pearson correlations and single factor model regressions, discovering an average BTC beta of 0.62, and isolating 12 companies, including Strategy (formerly MicroStrategy, MSTR), exhibiting a beta exceeding 1. We then classify firms into three groups reflecting their exposure to BTC, liquidity, and return co-movements. We use transfer entropy (TE) to capture the direction of information flow over time. Transfer entropy analysis consistently identifies BTC as the dominant information driver, with brief, announcement-driven feedback from stocks to BTC during major financial events. Our results highlight the critical need for dynamic hedging ratios that adapt to shifting information flows. These findings provide important insights for investors and managers regarding risk management and portfolio diversification in a period of growing integration of digital assets into corporate treasuries.</details> |
| 2025-05-20 | Quantum Reservoir Computing for Realized Volatility Forecasting | Qingyu Li, Chiranjib Mukhopadhyay, Abolfazl Bayat et.al. | [2505.13933](http://arxiv.org/abs/2505.13933) | **[link](https://github.com/leeqy1996/quantum-reservoir-computing-for-realized-volatility-forecasting)** | 18 pages, 8 figs, 4 tables. Comments/suggestions most welcome | <details><summary>Abstract (click to expand)</summary>Recent advances in quantum computing have demonstrated its potential to significantly enhance the analysis and forecasting of complex classical data. Among these, quantum reservoir computing has emerged as a particularly powerful approach, combining quantum computation with machine learning for modeling nonlinear temporal dependencies in high-dimensional time series. As with many data-driven disciplines, quantitative finance and econometrics can hugely benefit from emerging quantum technologies. In this work, we investigate the application of quantum reservoir computing for realized volatility forecasting. Our model employs a fully connected transverse-field Ising Hamiltonian as the reservoir with distinct input and memory qubits to capture temporal dependencies. The quantum reservoir computing approach is benchmarked against several econometric models and standard machine learning algorithms. The models are evaluated using multiple error metrics and the model confidence set procedures. To enhance interpretability and mitigate current quantum hardware limitations, we utilize wrapper-based forward selection for feature selection, identifying optimal subsets, and quantifying feature importance via Shapley values. Our results indicate that the proposed quantum reservoir approach consistently outperforms benchmark models across various metrics, highlighting its potential for financial forecasting despite existing quantum hardware constraints. This work serves as a proof-of-concept for the applicability of quantum computing in econometrics and financial analysis, paving the way for further research into quantum-enhanced predictive modeling as quantum hardware capabilities continue to advance.</details> |
| 2025-05-19 | Characterizing asymmetric and bimodal long-term financial return distributions through quantum walks | Stijn De Backer, Luis E. C. Rocha, Jan Ryckebusch et.al. | [2505.13019](http://arxiv.org/abs/2505.13019) |  | 24 pages, 11 figures, 2 tables | <details><summary>Abstract (click to expand)</summary>The analysis of logarithmic return distributions defined over large time scales is crucial for understanding the long-term dynamics of asset price movements. For large time scales of the order of two trading years, the anticipated Gaussian behavior of the returns often does not emerge, and their distributions often exhibit a high level of asymmetry and bimodality. These features are inadequately captured by the majority of classical models to address financial time series and return distributions. In the presented analysis, we use a model based on the discrete-time quantum walk to characterize the observed asymmetry and bimodality. The quantum walk distinguishes itself from a classical diffusion process by the occurrence of interference effects, which allows for the generation of bimodal and asymmetric probability distributions. By capturing the broader trends and patterns that emerge over extended periods, this analysis complements traditional short-term models and offers opportunities to more accurately describe the probabilistic structure underlying long-term financial decisions.</details> |
| 2025-05-19 | Hierarchical Representations for Evolving Acyclic Vector Autoregressions (HEAVe) | Cameron Cornell, Lewis Mitchell, Matthew Roughan et.al. | [2505.12806](http://arxiv.org/abs/2505.12806) |  |  | <details><summary>Abstract (click to expand)</summary>Causal networks offer an intuitive framework to understand influence structures within time series systems. However, the presence of cycles can obscure dynamic relationships and hinder hierarchical analysis. These networks are typically identified through multivariate predictive modelling, but enforcing acyclic constraints significantly increases computational and analytical complexity. Despite recent advances, there remains a lack of simple, flexible approaches that are easily tailorable to specific problem instances. We propose an evolutionary approach to fitting acyclic vector autoregressive processes and introduces a novel hierarchical representation that directly models structural elements within a time series system. On simulated datasets, our model retains most of the predictive accuracy of unconstrained models and outperforms permutation-based alternatives. When applied to a dataset of 100 cryptocurrency return series, our method generates acyclic causal networks capturing key structural properties of the unconstrained model. The acyclic networks are approximately sub-graphs of the unconstrained networks, and most of the removed links originate from low-influence nodes. Given the high levels of feature preservation, we conclude that this cryptocurrency price system functions largely hierarchically. Our findings demonstrate a flexible, intuitive approach for identifying hierarchical causal networks in time series systems, with broad applications to fields like econometrics and social network analysis.</details> |
| 2025-05-16 | Foundation Time-Series AI Model for Realized Volatility Forecasting | Anubha Goel, Puneet Pasricha, Martin Magris et.al. | [2505.11163](http://arxiv.org/abs/2505.11163) |  |  | <details><summary>Abstract (click to expand)</summary>Time series foundation models (FMs) have emerged as a popular paradigm for zero-shot multi-domain forecasting. These models are trained on numerous diverse datasets and claim to be effective forecasters across multiple different time series domains, including financial data. In this study, we evaluate the effectiveness of FMs, specifically the TimesFM model, for volatility forecasting, a core task in financial risk management. We first evaluate TimesFM in its pretrained (zero-shot) form, followed by our custom fine-tuning procedure based on incremental learning, and compare the resulting models against standard econometric benchmarks. While the pretrained model provides a reasonable baseline, our findings show that incremental fine-tuning, which allows the model to adapt to new financial return data over time, is essential for learning volatility patterns effectively. Fine-tuned variants not only improve forecast accuracy but also statistically outperform traditional models, as demonstrated through Diebold-Mariano and Giacomini-White tests. These results highlight the potential of foundation models as scalable and adaptive tools for financial forecasting-capable of delivering strong performance in dynamic market environments when paired with targeted fine-tuning strategies.</details> |
| 2025-05-15 | Reproducing the first and second moment of empirical degree distributions | Mattia Marzi, Francesca Giuffrida, Diego Garlaschelli et.al. | [2505.10373](http://arxiv.org/abs/2505.10373) |  | 14 pages, 7 figures | <details><summary>Abstract (click to expand)</summary>The study of probabilistic models for the analysis of complex networks represents a flourishing research field. Among the former, Exponential Random Graphs (ERGs) have gained increasing attention over the years. So far, only linear ERGs have been extensively employed to gain insight into the structural organisation of real-world complex networks. None, however, is capable of accounting for the variance of the empirical degree distribution. To this aim, non-linear ERGs must be considered. After showing that the usual mean-field approximation forces the degree-corrected version of the two-star model to degenerate, we define a fitness-induced variant of it. Such a `softened' model is capable of reproducing the sample variance, while retaining the explanatory power of its linear counterpart, within a purely canonical framework.</details> |
| 2025-04-28 | Mechanisms of information communication and market price movements. The case of SP 500 market | Inga Ivanova, Grzegorz Rzadkowski et.al. | [2505.09625](http://arxiv.org/abs/2505.09625) |  |  | <details><summary>Abstract (click to expand)</summary>In this paper we analyze how market prices change in response to information processing among the market participants and how non-linear information dynamics drive market price movement. We analyze historical data of the SP 500 market for the period 1950 -2025 using the logistic Continuous Wavelet Transformation method. This approach allows us to identify various patterns in market dynamics. These patterns are conceptualized using a new theory of reflexive communication of information in a market consisting of heterogeneous agents who assign meaning to information from different perspectives. This allows us to describe market dynamics and make forecasts of its development using the most general mechanisms of information circulation within the content-free approach.</details> |
| 2025-05-13 | An Efficient Multi-scale Leverage Effect Estimator under Dependent Microstructure Noise | Ziyang Xiong, Zhao Chen, Christina Dan Wang et.al. | [2505.08654](http://arxiv.org/abs/2505.08654) |  |  | <details><summary>Abstract (click to expand)</summary>Estimating the leverage effect from high-frequency data is vital but challenged by complex, dependent microstructure noise, often exhibiting non-Gaussian higher-order moments. This paper introduces a novel multi-scale framework for efficient and robust leverage effect estimation under such flexible noise structures. We develop two new estimators, the Subsampling-and-Averaging Leverage Effect (SALE) and the Multi-Scale Leverage Effect (MSLE), which adapt subsampling and multi-scale approaches holistically using a unique shifted window technique. This design simplifies the multi-scale estimation procedure and enhances noise robustness without requiring the pre-averaging approach. We establish central limit theorems and stable convergence, with MSLE achieving convergence rates of an optimal $n^{-1/4}$ and a near-optimal $n^{-1/9}$ for the noise-free and noisy settings, respectively. A cornerstone of our framework's efficiency is a specifically designed MSLE weighting strategy that leverages covariance structures across scales. This significantly reduces asymptotic variance and, critically, yields substantially smaller finite-sample errors than existing methods under both noise-free and realistic noisy settings. Extensive simulations and empirical analyses confirm the superior efficiency, robustness, and practical advantages of our approach.</details> |
| 2025-05-13 | Forecasting Intraday Volume in Equity Markets with Machine Learning | Mihai Cucuringu, Kang Li, Chao Zhang et.al. | [2505.08180](http://arxiv.org/abs/2505.08180) |  |  | <details><summary>Abstract (click to expand)</summary>This study focuses on forecasting intraday trading volumes, a crucial component for portfolio implementation, especially in high-frequency (HF) trading environments. Given the current scarcity of flexible methods in this area, we employ a suite of machine learning (ML) models enriched with numerous HF predictors to enhance the predictability of intraday trading volumes. Our findings reveal that intraday stock trading volume is highly predictable, especially with ML and considering commonality. Additionally, we assess the economic benefits of accurate volume forecasting through Volume Weighted Average Price (VWAP) strategies. The results demonstrate that precise intraday forecasting offers substantial advantages, providing valuable insights for traders to optimize their strategies.</details> |
| 2025-05-11 | Copula Analysis of Risk: A Multivariate Risk Analysis for VaR and CoVaR using Copulas and DCC-GARCH | Aryan Singh, Paul O Reilly, Daim Sharif et.al. | [2505.06950](http://arxiv.org/abs/2505.06950) | **[link](https://github.com/aryansingh920/copulas-in-time-series-financial-modelling)** | 15 pages, 12 figures, presented as part of the CS7DS1 - Data   Analytics module at Trinity College Dublin, May 2025 | <details><summary>Abstract (click to expand)</summary>A multivariate risk analysis for VaR and CVaR using different copula families is performed on historical financial time series fitted with DCC-GARCH models. A theoretical background is provided alongside a comparison of goodness-of-fit across different copula families to estimate the validity and effectiveness of approaches discussed.</details> |
| 2025-05-09 | Beyond the Mean: Limit Theory and Tests for Infinite-Mean Autoregressive Conditional Durations | Giuseppe Cavaliere, Thomas Mikosch, Anders Rahbek et.al. | [2505.06190](http://arxiv.org/abs/2505.06190) |  |  | <details><summary>Abstract (click to expand)</summary>Integrated autoregressive conditional duration (ACD) models serve as natural counterparts to the well-known integrated GARCH models used for financial returns. However, despite their resemblance, asymptotic theory for ACD is challenging and also not complete, in particular for integrated ACD. Central challenges arise from the facts that (i) integrated ACD processes imply durations with infinite expectation, and (ii) even in the non-integrated case, conventional asymptotic approaches break down due to the randomness in the number of durations within a fixed observation period. Addressing these challenges, we provide here unified asymptotic theory for the (quasi-) maximum likelihood estimator for ACD models; a unified theory which includes integrated ACD models. Based on the new results, we also provide a novel framework for hypothesis testing in duration models, enabling inference on a key empirical question: whether durations possess a finite or infinite expectation. We apply our results to high-frequency cryptocurrency ETF trading data. Motivated by parameter estimates near the integrated ACD boundary, we assess whether durations between trades in these markets have finite expectation, an assumption often made implicitly in the literature on point process models. Our empirical findings indicate infinite-mean durations for all the five cryptocurrencies examined, with the integrated ACD hypothesis rejected -- against alternatives with tail index less than one -- for four out of the five cryptocurrencies considered.</details> |
| 2025-05-16 | Why is the volatility of single stocks so much rougher than that of the S&P500? | Othmane Zarhali, Cecilia Aubrun, Emmanuel Bacry et.al. | [2505.02678](http://arxiv.org/abs/2505.02678) |  |  | <details><summary>Abstract (click to expand)</summary>The Nested factor model was introduced by Chicheportiche et al. to represent non-linear correlations between stocks. Stock returns are explained by a standard factor model, but the (log)-volatilities of factors and residuals are themselves decomposed into factor modes, with a common dominant volatility mode affecting both market and sector factors but also residuals. Here, we consider the case of a single factor where the only dominant log-volatility mode is rough, with a Hurst exponent $H \simeq 0.11$ and the log-volatility residuals are ''super-rough'', with $H \simeq 0$ . We demonstrate that such a construction naturally accounts for the somewhat surprising stylized fact reported by Wu et al. , where it has been observed that the Hurst exponents of stock indexes are large compared to those of individual stocks. We propose a statistical procedure to estimate the Hurst factor exponent from the stock returns dynamics together with theoretical guarantees of its consistency. We demonstrate the effectiveness of our approach through numerical experiments and apply it to daily stock data from the S&P500 index. The estimated roughness exponents for both the factor and idiosyncratic components validate the assumptions underlying our model.</details> |
| 2025-05-02 | Multiscale Causal Analysis of Market Efficiency via News Uncertainty Networks and the Financial Chaos Index | Masoud Ataei et.al. | [2505.01543](http://arxiv.org/abs/2505.01543) |  |  | <details><summary>Abstract (click to expand)</summary>This study evaluates the scale-dependent informational efficiency of stock markets using the Financial Chaos Index, a tensor-eigenvalue-based measure of realized volatility. Incorporating Granger causality and network-theoretic analysis across a range of economic, policy, and news-based uncertainty indices, we assess whether public information is efficiently incorporated into asset price fluctuations. Based on a 34-year time period from 1990 to 2023, at the daily frequency, the semi-strong form of the Efficient Market Hypothesis is rejected at the 1\% level of significance, indicating that asset price changes respond predictably to lagged news-based uncertainty. In contrast, at the monthly frequency, such predictive structure largely vanishes, supporting informational efficiency at coarser temporal resolutions. A structural analysis of the Granger causality network reveals that fiscal and monetary policy uncertainties act as core initiators of systemic volatility, while peripheral indices, such as those related to healthcare and consumer prices, serve as latent bridges that become activated under crisis conditions. These findings underscore the role of time-scale decomposition and structural asymmetries in diagnosing market inefficiencies and mapping the propagation of macro-financial uncertainty.</details> |
| 2025-05-02 | Towards modelling lifetime default risk: Exploring different subtypes of recurrent event Cox-regression models | Arno Botha, Tanja Verster, Bernard Scheepers et.al. | [2505.01044](http://arxiv.org/abs/2505.01044) |  | 9043 words, 23 pages, 11 figures | <details><summary>Abstract (click to expand)</summary>In the pursuit of modelling a loan's probability of default (PD) over its lifetime, repeat default events are often ignored when using Cox Proportional Hazard (PH) models. Excluding such events may produce biased and inaccurate PD-estimates, which can compromise financial buffers against future losses. Accordingly, we investigate a few subtypes of Cox-models that can incorporate recurrent default events. Using South African mortgage data, we explore both the Andersen-Gill (AG) and the Prentice-Williams-Peterson (PWP) spell-time models. These models are compared against a baseline that deliberately ignores recurrent events, called the time to first default (TFD) model. Models are evaluated using Harrell's c-statistic, adjusted Cox-Sell residuals, and a novel extension of time-dependent receiver operating characteristic (ROC) analysis. From these Cox-models, we demonstrate how to derive a portfolio-level term-structure of default risk, which is a series of marginal PD-estimates at each point of the average loan's lifetime. While the TFD- and PWP-models do not differ significantly across all diagnostics, the AG-model underperformed expectations. Depending on the prevalence of recurrent defaults, one may therefore safely ignore them when estimating lifetime default risk. Accordingly, our work enhances the current practice of using Cox-modelling in producing timeous and accurate PD-estimates under IFRS 9.</details> |
| 2025-04-29 | Scaling and shape of financial returns distributions modeled as conditionally independent random variables | Hern√°n Larralde, Roberto Mota Navarro et.al. | [2504.20488](http://arxiv.org/abs/2504.20488) |  |  | <details><summary>Abstract (click to expand)</summary>We show that assuming that the returns are independent when conditioned on the value of their variance (volatility), which itself varies in time randomly, then the distribution of returns is well described by the statistics of the sum of conditionally independent random variables. In particular, we show that the distribution of returns can be cast in a simple scaling form, and that its functional form is directly related to the distribution of the volatilities. This approach explains the presence of power-law tails in the returns as a direct consequence of the presence of a power law tail in the distribution of volatilities. It also provides the form of the distribution of Bitcoin returns, which behaves as a stretched exponential, as a consequence of the fact that the Bitcoin volatilities distribution is also closely described by a stretched exponential. We test our predictions with data from the S\&P 500 index, Apple and Paramount stocks; and Bitcoin.</details> |
| 2025-04-28 | Financial Data Analysis with Robust Federated Logistic Regression | Kun Yang, Nikhil Krishnan, Sanjeev R. Kulkarni et.al. | [2504.20250](http://arxiv.org/abs/2504.20250) | **[link](https://github.com/kun0906/flr)** |  | <details><summary>Abstract (click to expand)</summary>In this study, we focus on the analysis of financial data in a federated setting, wherein data is distributed across multiple clients or locations, and the raw data never leaves the local devices. Our primary focus is not only on the development of efficient learning frameworks (for protecting user data privacy) in the field of federated learning but also on the importance of designing models that are easier to interpret. In addition, we care about the robustness of the framework to outliers. To achieve these goals, we propose a robust federated logistic regression-based framework that strives to strike a balance between these goals. To verify the feasibility of our proposed framework, we carefully evaluate its performance not only on independently identically distributed (IID) data but also on non-IID data, especially in scenarios involving outliers. Extensive numerical results collected from multiple public datasets demonstrate that our proposed method can achieve comparable performance to those of classical centralized algorithms, such as Logistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary and multi-class classification tasks.</details> |
| 2025-04-28 | Compounding Effects in Leveraged ETFs: Beyond the Volatility Drag Paradigm | Chung-Han Hsieh, Jow-Ran Chang, Hui Hsiang Chen et.al. | [2504.20116](http://arxiv.org/abs/2504.20116) |  | Submitted for possible publication | <details><summary>Abstract (click to expand)</summary>A common belief is that leveraged ETFs (LETFs) suffer long-term performance decay due to \emph{volatility drag}. We show that this view is incomplete: LETF performance depends fundamentally on return autocorrelation and return dynamics. In markets with independent returns, LETFs exhibit positive expected compounding effects on their target multiples. In serially correlated markets, trends enhance returns, while mean reversion induces underperformance. With a unified framework incorporating AR(1) and AR-GARCH models, continuous-time regime switching, and flexible rebalancing frequencies, we demonstrate that return dynamics -- including return autocorrelation, volatility clustering, and regime persistence -- determine whether LETFs outperform or underperform their targets. Empirically, using about 20 years of SPDR S\&P~500 ETF and Nasdaq-100 ETF data, we confirm these theoretical predictions. Daily-rebalanced LETFs enhance returns in momentum-driven markets, whereas infrequent rebalancing mitigates losses in mean-reverting regimes.</details> |
| 2025-04-25 | Deep Learning vs. Black-Scholes: Option Pricing Performance on Brazilian Petrobras Stocks | Joao Felipe Gueiros, Hemanth Chandravamsi, Steven H. Frankel et.al. | [2504.20088](http://arxiv.org/abs/2504.20088) |  | 11 pages, 7 figures, 3 tables | <details><summary>Abstract (click to expand)</summary>This paper explores the use of deep residual networks for pricing European options on Petrobras, one of the world's largest oil and gas producers, and compares its performance with the Black-Scholes (BS) model. Using eight years of historical data from B3 (Brazilian Stock Exchange) collected via web scraping, a deep learning model was trained using a custom built hybrid loss function that incorporates market data and analytical pricing. The data for training and testing were drawn between the period spanning November 2016 to January 2025, using an 80-20 train-test split. The test set consisted of data from the final three months: November, December, and January 2025. The deep residual network model achieved a 64.3\% reduction in the mean absolute error for the 3-19 BRL (Brazilian Real) range when compared to the Black-Scholes model on the test set. Furthermore, unlike the Black-Scholes solution, which tends to decrease its accuracy for longer periods of time, the deep learning model performed accurately for longer expiration periods. These findings highlight the potential of deep learning in financial modeling, with future work focusing on specialized models for different price ranges.</details> |
| 2025-04-14 | Predictive AI with External Knowledge Infusion for Stocks | Ambedkar Dukkipati, Kawin Mayilvaghanan, Naveen Kumar Pallekonda et.al. | [2504.20058](http://arxiv.org/abs/2504.20058) |  |  | <details><summary>Abstract (click to expand)</summary>Fluctuations in stock prices are influenced by a complex interplay of factors that go beyond mere historical data. These factors, themselves influenced by external forces, encompass inter-stock dynamics, broader economic factors, various government policy decisions, outbreaks of wars, etc. Furthermore, all of these factors are dynamic and exhibit changes over time. In this paper, for the first time, we tackle the forecasting problem under external influence by proposing learning mechanisms that not only learn from historical trends but also incorporate external knowledge from temporal knowledge graphs. Since there are no such datasets or temporal knowledge graphs available, we study this problem with stock market data, and we construct comprehensive temporal knowledge graph datasets. In our proposed approach, we model relations on external temporal knowledge graphs as events of a Hawkes process on graphs. With extensive experiments, we show that learned dynamic representations effectively rank stocks based on returns across multiple holding periods, outperforming related baselines on relevant metrics.</details> |
| 2025-04-28 | Multi-Horizon Echo State Network Prediction of Intraday Stock Returns | Giovanni Ballarin, Jacopo Capra, Petros Dellaportas et.al. | [2504.19623](http://arxiv.org/abs/2504.19623) |  | 27 pages, 3 figures, 7 tables | <details><summary>Abstract (click to expand)</summary>Stock return prediction is a problem that has received much attention in the finance literature. In recent years, sophisticated machine learning methods have been shown to perform significantly better than ''classical'' prediction techniques. One downside of these approaches is that they are often very expensive to implement, for both training and inference, because of their high complexity. We propose a return prediction framework for intraday returns at multiple horizons based on Echo State Network (ESN) models, wherein a large portion of parameters are drawn at random and never trained. We show that this approach enjoys the benefits of recurrent neural network expressivity, inherently efficient implementation, and strong forecasting performance.</details> |
| 2025-04-26 | Phase Transitions in Financial Markets Using the Ising Model: A Statistical Mechanics Perspective | Bruno Giorgio et.al. | [2504.19050](http://arxiv.org/abs/2504.19050) |  |  | <details><summary>Abstract (click to expand)</summary>This dissertation investigates the ability of the Ising model to replicate statistical characteristics, or stylized facts, commonly observed in financial assets. The study specifically examines in the S&P500 index the following features: volatility clustering, negative skewness, heavy tails, the absence of autocorrelation in returns, and the presence of autocorrelation in absolute returns. A significant portion of the dissertation is dedicated to Ising model-based simulations. Due to the lack of an analytical or deterministic solution, the Monte Carlo method was employed to explore the model's statistical properties. The results demonstrate that the Ising model is capable of replicating the majority of the statistical features analyzed.</details> |
| 2025-04-26 | On Bitcoin Price Prediction | Gr√©gory Bournassenko et.al. | [2504.18982](http://arxiv.org/abs/2504.18982) |  |  | <details><summary>Abstract (click to expand)</summary>In recent years, cryptocurrencies have attracted growing attention from both private investors and institutions. Among them, Bitcoin stands out for its impressive volatility and widespread influence. This paper explores the predictability of Bitcoin's price movements, drawing a parallel with traditional financial markets. We examine whether the cryptocurrency market operates under the efficient market hypothesis (EMH) or if inefficiencies still allow opportunities for arbitrage. Our methodology combines theoretical reviews, empirical analyses, machine learning approaches, and time series modeling to assess the extent to which Bitcoin's price can be predicted. We find that while, in general, the Bitcoin market tends toward efficiency, specific conditions, including information asymmetries and behavioral anomalies, occasionally create exploitable inefficiencies. However, these opportunities remain difficult to systematically identify and leverage. Our findings have implications for both investors and policymakers, particularly regarding the regulation of cryptocurrency brokers and derivatives markets.</details> |
| 2025-04-26 | Impact of the COVID-19 pandemic on the financial market efficiency of price returns, absolute returns, and volatility increment: Evidence from stock and cryptocurrency markets | Tetsuya Takaishi et.al. | [2504.18960](http://arxiv.org/abs/2504.18960) |  | 20 pages, 10 figures | <details><summary>Abstract (click to expand)</summary>This study examines the impact of the coronavirus disease 2019 (COVID-19) pandemic on market efficiency by analyzing three time series -- price returns, absolute returns, and volatility increments -- in stock (Deutscher Aktienindex, Nikkei 225, Shanghai Stock Exchange (SSE), and Volatility Index) and cryptocurrency (Bitcoin and Ethereum) markets. The effect is found to vary by asset class and market. In the stock market, while the pandemic did not influence the Hurst exponent of volatility increments, it affected that of returns and absolute returns (except in the SSE, where returns remained unaffected). In the cryptocurrency market, the pandemic did not alter the Hurst exponent for any time series but influenced the strength of multifractality in returns and absolute returns. Some Hurst exponent time series exhibited a gradual decline over time, complicating the assessment of pandemic-related effects. Consequently, segmented analyses by pandemic periods may erroneously suggest an impact, warranting caution in period-based studies.</details> |
| 2025-04-26 | Modeling Regime Structure and Informational Drivers of Stock Market Volatility via the Financial Chaos Index | Masoud Ataei et.al. | [2504.18958](http://arxiv.org/abs/2504.18958) |  |  | <details><summary>Abstract (click to expand)</summary>This paper investigates the structural dynamics of stock market volatility through the Financial Chaos Index, a tensor- and eigenvalue-based measure designed to capture realized volatility via mutual fluctuations among asset prices. Motivated by empirical evidence of regime-dependent volatility behavior and perceptual time dilation during financial crises, we develop a regime-switching framework based on the Modified Lognormal Power-Law distribution. Analysis of the FCIX from January 1990 to December 2023 identifies three distinct market regimes, low-chaos, intermediate-chaos, and high-chaos, each characterized by differing levels of systemic stress, statistical dispersion and persistence characteristics. Building upon the segmented regime structure, we further examine the informational forces that shape forward-looking market expectations. Using sentiment-based predictors derived from the Equity Market Volatility tracker, we employ an elastic net regression model to forecast implied volatility, as proxied by the VIX index. Our findings indicate that shifts in macroeconomic, financial, policy, and geopolitical uncertainty exhibit strong predictive power for volatility dynamics across regimes. Together, these results offer a unified empirical perspective on how systemic uncertainty governs both the realized evolution of financial markets and the anticipatory behavior embedded in implied volatility measures.</details> |
| 2025-04-23 | Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models | Fredy Pokou, Jules Sadefo Kamdem, Fran√ßois Benhmad et.al. | [2504.16635](http://arxiv.org/abs/2504.16635) |  |  | <details><summary>Abstract (click to expand)</summary>In an environment of increasingly volatile financial markets, the accurate estimation of risk remains a major challenge. Traditional econometric models, such as GARCH and its variants, are based on assumptions that are often too rigid to adapt to the complexity of the current market dynamics. To overcome these limitations, we propose a hybrid framework for Value-at-Risk (VaR) estimation, combining GARCH volatility models with deep reinforcement learning. Our approach incorporates directional market forecasting using the Double Deep Q-Network (DDQN) model, treating the task as an imbalanced classification problem. This architecture enables the dynamic adjustment of risk-level forecasts according to market conditions. Empirical validation on daily Eurostoxx 50 data covering periods of crisis and high volatility shows a significant improvement in the accuracy of VaR estimates, as well as a reduction in the number of breaches and also in capital requirements, while respecting regulatory risk thresholds. The ability of the model to adjust risk levels in real time reinforces its relevance to modern and proactive risk management.</details> |
| 2025-04-22 | Modeling and Forecasting Realized Volatility with Multivariate Fractional Brownian Motion | Markus Bibinger, Jun Yu, Chen Zhang et.al. | [2504.15985](http://arxiv.org/abs/2504.15985) |  |  | <details><summary>Abstract (click to expand)</summary>A multivariate fractional Brownian motion (mfBm) with component-wise Hurst exponents is used to model and forecast realized volatility. We investigate the interplay between correlation coefficients and Hurst exponents and propose a novel estimation method for all model parameters, establishing consistency and asymptotic normality of the estimators. Additionally, we develop a time-reversibility test, which is typically not rejected by real volatility data. When the data-generating process is a time-reversible mfBm, we derive optimal forecasting formulae and analyze their properties. A key insight is that an mfBm with different Hurst exponents and non-zero correlations can reduce forecasting errors compared to a one-dimensional model. Consistent with optimal forecasting theory, out-of-sample forecasts using the time-reversible mfBm show improvements over univariate fBm, particularly when the estimated Hurst exponents differ significantly. Empirical results demonstrate that mfBm-based forecasts outperform the (vector) HAR model.</details> |
| 2025-04-22 | Learning the Spoofability of Limit Order Books With Interpretable Probabilistic Neural Networks | Timoth√©e Fabre, Damien Challet et.al. | [2504.15908](http://arxiv.org/abs/2504.15908) |  | 22 pages | <details><summary>Abstract (click to expand)</summary>This paper investigates real-time detection of spoofing activity in limit order books, focusing on cryptocurrency centralized exchanges. We first introduce novel order flow variables based on multi-scale Hawkes processes that account both for the size and placement distance from current best prices of new limit orders. Using a Level-3 data set, we train a neural network model to predict the conditional probability distribution of mid price movements based on these features. Our empirical analysis highlights the critical role of the posting distance of limit orders in the price formation process, showing that spoofing detection models that do not take the posting distance into account are inadequate to describe the data. Next, we propose a spoofing detection framework based on the probabilistic market manipulation gain of a spoofing agent and use the previously trained neural network to compute the expected gain. Running this algorithm on all submitted limit orders in the period 2024-12-04 to 2024-12-07, we find that 31% of large orders could spoof the market. Because of its simple neuronal architecture, our model can be run in real time. This work contributes to enhancing market integrity by providing a robust tool for monitoring and mitigating spoofing in both cryptocurrency exchanges and traditional financial markets.</details> |
| 2025-05-03 | Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios | JD Opdyke et.al. | [2504.15268](http://arxiv.org/abs/2504.15268) |  |  | <details><summary>Abstract (click to expand)</summary>We live in a multivariate world, and effective modeling of financial portfolios, including their construction, allocation, forecasting, and risk analysis, simply is not possible without explicitly modeling the dependence structure of their assets. Dependence structure can drive portfolio results more than the combined effects of other parameters in investment and risk models, but the literature provides relatively little to define the finite-sample distributions of dependence measures in useable and useful ways under challenging, real-world financial data conditions. Yet this is exactly what is needed to make valid inferences about their estimates, and to use these inferences for essential purposes such as hypothesis testing, dynamic monitoring, realistic and granular scenario and reverse scenario analyses, and mitigating the effects of correlation breakdowns during market upheavals. This work develops a new and straightforward method, Nonparametric Angles-based Correlation (NAbC), for defining the finite-sample distributions of any dependence measure whose matrix of pairwise associations is positive definite (e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). The solution remains valid under marginal asset distributions characterized by notably different and varying degrees of serial correlation, non-stationarity, heavy-tailedness, and asymmetry. Importantly, NAbCs p-values and confidence intervals remain analytically consistent at both the matrix level and the pairwise cell level. Finally, NAbC maintains validity even when selected cells in the matrix are frozen for a given scenario or stress test, thus enabling flexible, granular, and realistic scenarios. NAbC stands alone in providing all of these capabilities simultaneously, and should prove to be a very useful means by which we can better understand and manage financial portfolios in our multivariate world.</details> |
| 2025-04-20 | The Memorization Problem: Can We Trust LLMs' Economic Forecasts? | Alejandro Lopez-Lira, Yuehua Tang, Mingyin Zhu et.al. | [2504.14765](http://arxiv.org/abs/2504.14765) |  |  | <details><summary>Abstract (click to expand)</summary>Large language models (LLMs) cannot be trusted for economic forecasts during periods covered by their training data. We provide the first systematic evaluation of LLMs' memorization of economic and financial data, including major economic indicators, news headlines, stock returns, and conference calls. Our findings show that LLMs can perfectly recall the exact numerical values of key economic variables from before their knowledge cutoff dates. This recall appears to be randomly distributed across different dates and data types. This selective perfect memory creates a fundamental issue -- when testing forecasting capabilities before their knowledge cutoff dates, we cannot distinguish whether LLMs are forecasting or simply accessing memorized data. Explicit instructions to respect historical data boundaries fail to prevent LLMs from achieving recall-level accuracy in forecasting tasks. Further, LLMs seem exceptional at reconstructing masked entities from minimal contextual clues, suggesting that masking provides inadequate protection against motivated reasoning. Our findings raise concerns about using LLMs to forecast historical data or backtest trading strategies, as their apparent predictive success may merely reflect memorization rather than genuine economic insight. Any application where future knowledge would change LLMs' outputs can be affected by memorization. In contrast, consistent with the absence of data contamination, LLMs cannot recall data after their knowledge cutoff date.</details> |
| 2025-04-21 | Deep Learning Models Meet Financial Data Modalities | Kasymkhan Khubiev, Mikhail Semenov et.al. | [2504.13521](http://arxiv.org/abs/2504.13521) |  | 15 pages, 14 images, 7 tables | <details><summary>Abstract (click to expand)</summary>Algorithmic trading relies on extracting meaningful signals from diverse financial data sources, including candlestick charts, order statistics on put and canceled orders, traded volume data, limit order books, and news flow. While deep learning has demonstrated remarkable success in processing unstructured data and has significantly advanced natural language processing, its application to structured financial data remains an ongoing challenge. This study investigates the integration of deep learning models with financial data modalities, aiming to enhance predictive performance in trading strategies and portfolio optimization. We present a novel approach to incorporating limit order book analysis into algorithmic trading by developing embedding techniques and treating sequential limit order book snapshots as distinct input channels in an image-based representation. Our methodology for processing limit order book data achieves state-of-the-art performance in high-frequency trading algorithms, underscoring the effectiveness of deep learning in financial applications.</details> |
| 2025-04-18 | Target search optimization by threshold resetting | Arup Biswas, Satya N Majumdar, Arnab Pal et.al. | [2504.13501](http://arxiv.org/abs/2504.13501) |  |  | <details><summary>Abstract (click to expand)</summary>We introduce a new class of first passage time optimization driven by threshold resetting, inspired by many natural processes where crossing a critical limit triggers failure, degradation or transition. In here, search agents are collectively reset when a threshold is reached, creating event-driven, system-coupled simultaneous resets that induce long-range interactions. We develop a unified framework to compute search times for these correlated stochastic processes, with ballistic searchers as a key example uncovering diverse optimization behaviors. A cost function, akin to breakdown penalties, reveals that optimal resetting can forestall larger losses. This formalism generalizes to broader stochastic systems with multiple degrees of freedom.</details> |
| 2025-04-02 | BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models | Sohom Ghosh, Sudip Kumar Naskar et.al. | [2504.13189](http://arxiv.org/abs/2504.13189) | **[link](https://huggingface.co/datasets/sohomghosh/BASIR_Budget_Assisted_Sectoral_Impact_Ranking)** | The codes and the datasets can be accessed from   <https://huggingface.co/datasets/sohomghosh/BASIR_Budget_Assisted_Sectoral_Impact_Ranking/tree/main/> | <details><summary>Abstract (click to expand)</summary>Government fiscal policies, particularly annual union budgets, exert significant influence on financial markets. However, real-time analysis of budgetary impacts on sector-specific equity performance remains methodologically challenging and largely unexplored. This study proposes a framework to systematically identify and rank sectors poised to benefit from India's Union Budget announcements. The framework addresses two core tasks: (1) multi-label classification of excerpts from budget transcripts into 81 predefined economic sectors, and (2) performance ranking of these sectors. Leveraging a comprehensive corpus of Indian Union Budget transcripts from 1947 to 2025, we introduce BASIR (Budget-Assisted Sectoral Impact Ranking), an annotated dataset mapping excerpts from budgetary transcripts to sectoral impacts. Our architecture incorporates fine-tuned embeddings for sector identification, coupled with language models that rank sectors based on their predicted performances. Our results demonstrate 0.605 F1-score in sector classification, and 0.997 NDCG score in predicting ranks of sectors based on post-budget performances. The methodology enables investors and policymakers to quantify fiscal policy impacts through structured, data-driven insights, addressing critical gaps in manual analysis. The annotated dataset has been released under CC-BY-NC-SA-4.0 license to advance computational economics research.</details> |
| 2025-04-17 | Classification-Based Analysis of Price Pattern Differences Between Cryptocurrencies and Stocks | Yu Zhang, Zelin Wu, Claudio Tessone et.al. | [2504.12771](http://arxiv.org/abs/2504.12771) |  |  | <details><summary>Abstract (click to expand)</summary>Cryptocurrencies are digital tokens built on blockchain technology, with thousands actively traded on centralized exchanges (CEXs). Unlike stocks, which are backed by real businesses, cryptocurrencies are recognized as a distinct class of assets by researchers. How do investors treat this new category of asset in trading? Are they similar to stocks as an investment tool for investors? We answer these questions by investigating cryptocurrencies' and stocks' price time series which can reflect investors' attitudes towards the targeted assets. Concretely, we use different machine learning models to classify cryptocurrencies' and stocks' price time series in the same period and get an extremely high accuracy rate, which reflects that cryptocurrency investors behave differently in trading from stock investors. We then extract features from these price time series to explain the price pattern difference, including mean, variance, maximum, minimum, kurtosis, skewness, and first to third-order autocorrelation, etc., and then use machine learning methods including logistic regression (LR), random forest (RF), support vector machine (SVM), etc. for classification. The classification results show that these extracted features can help to explain the price time series pattern difference between cryptocurrencies and stocks.</details> |
| 2025-04-13 | Integrated GARCH-GRU in Financial Volatility Forecasting | Jingyi Wei, Steve Yang, Zhenyu Cui et.al. | [2504.09380](http://arxiv.org/abs/2504.09380) |  |  | <details><summary>Abstract (click to expand)</summary>In this study, we propose a novel integrated Generalized Autoregressive Conditional Heteroskedasticity-Gated Recurrent Unit (GARCH-GRU) model for financial volatility modeling and forecasting. The model embeds the GARCH(1,1) formulation directly into the GRU cell architecture, yielding a unified recurrent unit that jointly captures both traditional econometric properties and complex temporal dynamics. This hybrid structure leverages the strengths of GARCH in modeling key stylized facts of financial volatility, such as clustering and persistence, while utilizing the GRU's capacity to learn nonlinear dependencies from sequential data. Compared to the GARCH-LSTM counterpart, the GARCH-GRU model demonstrates superior computational efficiency, requiring significantly less training time, while maintaining and improving forecasting accuracy. Empirical evaluation across multiple financial datasets confirms the model's robust outperformance in terms of mean squared error (MSE) and mean absolute error (MAE) relative to a range of benchmarks, including standard neural networks, alternative hybrid architectures, and classical GARCH-type models. As an application, we compute Value-at-Risk (VaR) using the model's volatility forecasts and observe lower violation ratios, further validating the predictive reliability of the proposed framework in practical risk management settings.</details> |
| 2025-04-12 | On the rate of convergence of estimating the Hurst parameter of rough stochastic volatility models | Xiyue Han, Alexander Schied et.al. | [2504.09276](http://arxiv.org/abs/2504.09276) |  | 10 pages | <details><summary>Abstract (click to expand)</summary>In [8], easily computable scale-invariant estimator $\widehat{\mathscr{R}}^s_n$ was constructed to estimate the Hurst parameter of the drifted fractional Brownian motion $X$ from its antiderivative. This paper extends this convergence result by proving that $\widehat{\mathscr{R}}^s_n$ also consistently estimates the Hurst parameter when applied to the antiderivative of $g \circ X$ for a general nonlinear function $g$ . We also establish an almost sure rate of convergence in this general setting. Our result applies, in particular, to the estimation of the Hurst parameter of a wide class of rough stochastic volatility models from discrete observations of the integrated variance, including the fractional stochastic volatility model.</details> |
| 2025-04-11 | International Financial Markets Through 150 Years: Evaluating Stylized Facts | Sara A. Safari, Maximilian Janisch, Thomas Leh√©ricy et.al. | [2504.08611](http://arxiv.org/abs/2504.08611) |  | 44 pages, 34 figures | <details><summary>Abstract (click to expand)</summary>In the theory of financial markets, a stylized fact is a qualitative summary of a pattern in financial market data that is observed across multiple assets, asset classes and time horizons. In this article, we test a set of eleven stylized facts for financial market data. Our main contribution is to consider a broad range of geographical regions across Asia, continental Europe, and the US over a time period of 150 years, as well as two of the most traded cryptocurrencies, thus providing insights into the robustness and generalizability of commonly known stylized facts.</details> |
| 2025-04-09 | Polyspectral Mean based Time Series Clustering of Indian Stock Market | Dhrubajyoti Ghosh et.al. | [2504.07021](http://arxiv.org/abs/2504.07021) |  | Published in Discover Data | <details><summary>Abstract (click to expand)</summary>In this study, we employ k-means clustering algorithm of polyspectral means to analyze 49 stocks in the Indian stock market. We have used spectral and bispectral information obtained from the data, by using spectral and bispectral means with different weight functions that will give us varying insights into the temporal patterns of the stocks. In particular, the higher order polyspectral means can provide significantly more information than what we can gather from power spectra, and can also unveil nonlinear trends in a time series. Through rigorous analysis, we identify five distinctive clusters, uncovering nuanced market structures. Notably, one cluster emerges as that of a conglomerate powerhouse, featuring ADANI, BIRLA, TATA, and unexpectedly, government-owned bank SBI. Another cluster spotlights the IT sector with WIPRO and TCS, while a third combines private banks, government entities, and RELIANCE. The final cluster comprises publicly traded companies with dispersed ownership. Such clustering of stocks sheds light on intricate financial relationships within the stock market, providing valuable insights for investors and analysts navigating the dynamic landscape of the Indian stock market.</details> |
| 2025-04-09 | Diffusion Factor Models: Generating High-Dimensional Returns with Factor Structure | Minshuo Chen, Renyuan Xu, Yumin Xu et.al. | [2504.06566](http://arxiv.org/abs/2504.06566) | **[link](https://github.com/xymmmm00/diffusion_factor_model)** |  | <details><summary>Abstract (click to expand)</summary>Financial scenario simulation is essential for risk management and portfolio optimization, yet it remains challenging especially in high-dimensional and small data settings common in finance. We propose a diffusion factor model that integrates latent factor structure into generative diffusion processes, bridging econometrics with modern generative AI to address the challenges of the curse of dimensionality and data scarcity in financial simulation. By exploiting the low-dimensional factor structure inherent in asset returns, we decompose the score function--a key component in diffusion models--using time-varying orthogonal projections, and this decomposition is incorporated into the design of neural network architectures. We derive rigorous statistical guarantees, establishing nonasymptotic error bounds for both score estimation at O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4} n^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather than the number of assets d, surpassing the dimension-dependent limits in the classical nonparametric statistics literature and making the framework viable for markets with thousands of assets. Numerical studies confirm superior performance in latent subspace recovery under small data regimes. Empirical analysis demonstrates the economic significance of our framework in constructing mean-variance optimal portfolios and factor portfolios. This work presents the first theoretical integration of factor structure with diffusion models, offering a principled approach for high-dimensional financial simulation with limited data.</details> |
| 2025-03-20 | Financial Analysis: Intelligent Financial Data Analysis System Based on LLM-RAG | Jingru Wang, Wen Ding, Xiaotong Zhu et.al. | [2504.06279](http://arxiv.org/abs/2504.06279) |  |  | <details><summary>Abstract (click to expand)</summary>In the modern financial sector, the exponential growth of data has made efficient and accurate financial data analysis increasingly crucial. Traditional methods, such as statistical analysis and rule-based systems, often struggle to process and derive meaningful insights from complex financial information effectively. These conventional approaches face inherent limitations in handling unstructured data, capturing intricate market patterns, and adapting to rapidly evolving financial contexts, resulting in reduced accuracy and delayed decision-making processes. To address these challenges, this paper presents an intelligent financial data analysis system that integrates Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) technology. Our system incorporates three key components: a specialized preprocessing module for financial data standardization, an efficient vector-based storage and retrieval system, and a RAG-enhanced query processing module. Using the NASDAQ financial fundamentals dataset from 2010 to 2023, we conducted comprehensive experiments to evaluate system performance. Results demonstrate significant improvements across multiple metrics: the fully optimized configuration (gpt-3.5-turbo-1106+RAG) achieved 78.6% accuracy and 89.2% recall, surpassing the baseline model by 23 percentage points in accuracy while reducing response time by 34.8%. The system also showed enhanced efficiency in handling complex financial queries, though with a moderate increase in memory utilization. Our findings validate the effectiveness of integrating RAG technology with LLMs for financial analysis tasks and provide valuable insights for future developments in intelligent financial data processing systems.</details> |
| 2025-04-08 | A Mean-Reverting Model of Exchange Rate Risk Premium Using Ornstein-Uhlenbeck Dynamics | SeungJae Hwang et.al. | [2504.06028](http://arxiv.org/abs/2504.06028) |  | 7 pages, 5 figures. Includes empirical backtesting of a   continuous-time stochastic model. Independent undergraduate research | <details><summary>Abstract (click to expand)</summary>This paper examines the empirical failure of uncovered interest parity (UIP) and proposes a structural explanation based on a mean-reverting risk premium. We define a realized premium as the deviation between observed exchange rate returns and the interest rate differential, and demonstrate its strong mean-reverting behavior across multiple horizons. Motivated by this pattern, we model the risk premium using an Ornstein-Uhlenbeck (OU) process embedded within a stochastic differential equation for the exchange rate.   Our model yields closed-form approximations for future exchange rate distributions, which we evaluate using coverage-based backtesting. Applied to USD/KRW data from 2010 to 2025, the model shows strong predictive performance at both short-term and long-term horizons, while underperforming at intermediate (3-month) horizons and showing conservative behavior in the tails of long-term forecasts. These results suggest that exchange rate deviations from UIP may reflect structured, forecastable dynamics rather than pure noise, and point to future modeling improvements via regime-switching or time-varying volatility.</details> |
| 2025-04-08 | Financial resilience of agricultural and food production companies in Spain: A compositional cluster analysis of the impact of the Ukraine-Russia war (2021-2023) | Mike Hernandez Romero, Germ√† Coenders et.al. | [2504.05912](http://arxiv.org/abs/2504.05912) |  |  | <details><summary>Abstract (click to expand)</summary>This study analyzes the financial resilience of agricultural and food production companies in Spain amid the Ukraine-Russia war using cluster analysis based on financial ratios. This research utilizes centered log-ratios to transform financial ratios for compositional data analysis. The dataset comprises financial information from 1197 firms in Spain's agricultural and food sectors over the period 2021-2023. The analysis reveals distinct clusters of firms with varying financial performance, characterized by metrics of solvency and profitability. The results highlight an increase in resilient firms by 2023, underscoring sectoral adaptation to the conflict's economic challenges. These findings together provide insights for stakeholders and policymakers to improve sectorial stability and strategic planning.</details> |
| 2025-04-03 | Online Multivariate Regularized Distributional Regression for High-dimensional Probabilistic Electricity Price Forecasting | Simon Hirsch et.al. | [2504.02518](http://arxiv.org/abs/2504.02518) | **[link](https://github.com/simon-hirsch/online-mv-distreg)** | 35 pages incl. Appendix, 9 Figures | <details><summary>Abstract (click to expand)</summary>Probabilistic electricity price forecasting (PEPF) is a key task for market participants in short-term electricity markets. The increasing availability of high-frequency data and the need for real-time decision-making in energy markets require online estimation methods for efficient model updating. We present an online, multivariate, regularized distributional regression model, allowing for the modeling of all distribution parameters conditional on explanatory variables. Our approach is based on the combination of the multivariate distributional regression and an efficient online learning algorithm based on online coordinate descent for LASSO-type regularization. Additionally, we propose to regularize the estimation along a path of increasingly complex dependence structures of the multivariate distribution, allowing for parsimonious estimation and early stopping. We validate our approach through one of the first forecasting studies focusing on multivariate probabilistic forecasting in the German day-ahead electricity market while using only online estimation methods. We compare our approach to online LASSO-ARX-models with adaptive marginal distribution and to online univariate distributional models combined with an adaptive Copula. We show that the multivariate distributional regression, which allows modeling all distribution parameters - including the mean and the dependence structure - conditional on explanatory variables such as renewable in-feed or past prices provide superior forecasting performance compared to modeling of the marginals only and keeping a static/unconditional dependence structure. Additionally, online estimation yields a speed-up by a factor of 80 to over 400 times compared to batch fitting.</details> |
| 2025-03-24 | Cryptocurrency Time Series on the Binary Complexity-Entropy Plane: Ranking Efficiency from the Perspective of Complex Systems | Erveton P. Pinto, Marcelo A. Pires, Rone N. da Silva et.al. | [2504.01974](http://arxiv.org/abs/2504.01974) |  | 12 pages, 8 figures, 2 tables and 3 appendices | <details><summary>Abstract (click to expand)</summary>We report the first application of a tailored Complexity-Entropy Plane designed for binary sequences and structures. We do so by considering the daily up/down price fluctuations of the largest cryptocurrencies in terms of capitalization (stable-coins excluded) that are worth $circa \,\, 90 \%$ of the total crypto market capitalization. With that, we focus on the basic elements of price motion that compare with the random walk backbone features associated with mathematical properties of the Efficient Market Hypothesis. From the location of each crypto on the Binary Complexity-Plane (BiCEP) we define an inefficiency score, $\mathcal I$, and rank them accordingly. The results based on the BiCEP analysis, which we substantiate with statistical testing, indicate that only Shiba Inu (SHIB) is significantly inefficient, whereas the largest stake of crypto trading is reckoned to operate in close-to-efficient conditions. Generically, our $\mathcal I$ -based ranking hints the design and consensus architecture of a crypto is at least as relevant to efficiency as the features that are usually taken into account in the appraisal of the efficiency of financial instruments, namely canonical fiat money. Lastly, this set of results supports the validity of the binary complexity analysis.</details> |
| 2025-04-04 | What Can 240,000 New Credit Transactions Tell Us About the Impact of NGEU Funds? | Alvaro Ortiz, Tomasa Rodrigo, David Sarasa et.al. | [2504.01964](http://arxiv.org/abs/2504.01964) |  |  | <details><summary>Abstract (click to expand)</summary>Using a panel data local projections model and controlling for firm characteristics, procurement bid attributes, and macroeconomic conditions, the study estimates the dynamic effects of procurement awards on new lending, a more precise measure than the change in the stock of credit. The analysis further examines heterogeneity in credit responses based on firm size, industry, credit maturity, and value chain position of the firms. The empirical evidence confirms that public procurement awards significantly increase new lending, with NGEU-funded contracts generating stronger credit expansion than traditional procurement during the recent period. The results show that the impact of NGEU procurement programs aligns closely with historical procurement impacts, with differences driven mainly by lower utilization rates. Moreover, integrating high-frequency financial data with procurement records highlights the potential of Big Data in refining public policy design.</details> |
| 2025-03-31 | Asymmetry in Distributions of Accumulated Gains and Losses in Stock Returns | Hamed Farahani, R. A. Serota et.al. | [2503.24241](http://arxiv.org/abs/2503.24241) |  | 16 pages, 17 figures, 3 tables | <details><summary>Abstract (click to expand)</summary>We study decades-long historic distributions of accumulated S\&P500 returns, from daily returns to those over several weeks. The time series of the returns emphasize major upheavals in the markets -- Black Monday, Tech Bubble, Financial Crisis and Covid Pandemic -- which are reflected in the tail ends of the distributions. De-trending the overall gain, we concentrate on comparing distributions of gains and losses. Specifically, we compare the tails of the distributions, which are believed to exhibit power-law behavior and possibly contain outliers. Towards this end we find confidence intervals of the linear fits of the tails of the complementary cumulative distribution functions on a log-log scale, as well as conduct a statistical U-test in order to detect outliers. We also study probability density functions of the full distributions of the returns with the emphasis on their asymmetry. The key empirical observations are that the mean of de-trended distributions increases near-linearly with the number of days of accumulation while the overall skew is negative -- consistent with the heavier tails of losses -- and depends little on the number of days of accumulation. At the same time the variance of the distributions exhibits near-perfect linear dependence on the number of days of accumulation, that is it remains constant if scaled to the latter. Finally, we discuss the theoretical framework for understanding accumulated returns. Our main conclusion is that the current state of theory, which predicts symmetric or near-symmetric distributions of returns cannot explain the aggregate of empirical results.</details> |
| 2025-03-31 | A cost of capital approach to determining the LGD discount rate | Janette Larney, Arno Botha, Gerrit Lodewicus Grobler et.al. | [2503.23992](http://arxiv.org/abs/2503.23992) |  | 7374 words, 5 figures | <details><summary>Abstract (click to expand)</summary>Loss Given Default (LGD) is a key risk parameter in determining a bank's regulatory capital. During LGD-estimation, realised recovery cash flows are to be discounted at an appropriate rate. Regulatory guidance mandates that this rate should allow for the time value of money, as well as include a risk premium that reflects the "undiversifiable risk" within these recoveries. Having extensively reviewed earlier methods of determining this rate, we propose a new approach that is inspired by the cost of capital approach from the Solvency II regulatory regime. Our method involves estimating a market-consistent price for a portfolio of defaulted loans, from which an associated discount rate may be inferred. We apply this method to mortgage and personal loans data from a large South African bank. The results reveal the main drivers of the discount rate to be the mean and variance of these recoveries, as well as the bank's cost of capital in excess of the risk-free rate. Our method therefore produces a discount rate that reflects both the undiversifiable risk of recovery recoveries and the time value of money, thereby satisfying regulatory requirements. This work can subsequently enhance the LGD-component within the modelling of both regulatory and economic capital.</details> |
| 2025-03-14 | Bridging Language Models and Financial Analysis | Alejandro Lopez-Lira, Jihoon Kwon, Sangwoon Yoon et.al. | [2503.22693](http://arxiv.org/abs/2503.22693) |  | 28 pages | <details><summary>Abstract (click to expand)</summary>The rapid advancements in Large Language Models (LLMs) have unlocked transformative possibilities in natural language processing, particularly within the financial sector. Financial data is often embedded in intricate relationships across textual content, numerical tables, and visual charts, posing challenges that traditional methods struggle to address effectively. However, the emergence of LLMs offers new pathways for processing and analyzing this multifaceted data with increased efficiency and insight. Despite the fast pace of innovation in LLM research, there remains a significant gap in their practical adoption within the finance industry, where cautious integration and long-term validation are prioritized. This disparity has led to a slower implementation of emerging LLM techniques, despite their immense potential in financial applications. As a result, many of the latest advancements in LLM technology remain underexplored or not fully utilized in this domain. This survey seeks to bridge this gap by providing a comprehensive overview of recent developments in LLM research and examining their applicability to the financial sector. Building on previous survey literature, we highlight several novel LLM methodologies, exploring their distinctive capabilities and their potential relevance to financial data analysis. By synthesizing insights from a broad range of studies, this paper aims to serve as a valuable resource for researchers and practitioners, offering direction on promising research avenues and outlining future opportunities for advancing LLM applications in finance.</details> |
| 2025-03-27 | From Deep Learning to LLMs: A survey of AI in Quantitative Investment | Bokai Cao, Saizhuo Wang, Xinyi Lin et.al. | [2503.21422](http://arxiv.org/abs/2503.21422) |  |  | <details><summary>Abstract (click to expand)</summary>Quantitative investment (quant) is an emerging, technology-driven approach in asset management, increasingy shaped by advancements in artificial intelligence. Recent advances in deep learning and large language models (LLMs) for quant finance have improved predictive modeling and enabled agent-based automation, suggesting a potential paradigm shift in this field. In this survey, taking alpha strategy as a representative example, we explore how AI contributes to the quantitative investment pipeline. We first examine the early stage of quant research, centered on human-crafted features and traditional statistical models with an established alpha pipeline. We then discuss the rise of deep learning, which enabled scalable modeling across the entire pipeline from data processing to order execution. Building on this, we highlight the emerging role of LLMs in extending AI beyond prediction, empowering autonomous agents to process unstructured data, generate alphas, and support self-iterative workflows.</details> |
| 2025-03-27 | Dynamic Asset Pricing Theory for Life Contingent Risks | Patrick Ling et.al. | [2503.21256](http://arxiv.org/abs/2503.21256) |  |  | <details><summary>Abstract (click to expand)</summary>Although the valuation of life contingent assets has been thoroughly investigated under the framework of mathematical statistics, little financial economics research pays attention to the pricing of these assets in a non-arbitrage, complete market. In this paper, we first revisit the Fundamental Theorem of Asset Pricing (FTAP) and the short proof of it. Then we point out that discounted asset price is a martingale only when dividends are zero under all random states of the world, using a simple proof based on pricing kernel. Next, we apply Fundamental Theorem of Asset Pricing (FTAP) to find valuation formula for life contingent assets including life insurance policies and life contingent annuities. Last but not least, we state the assumption of static portfolio in a dynamic economy, and clarify the FTAP that accommodates the valuation of a portfolio of life contingent policies.</details> |
| 2025-03-01 | Ornstein-Uhlenbeck Process for Horse Race Betting: A Micro-Macro Analysis of Herding and Informed Bettors | Tomoya Sugawara, Shintaro Mori et.al. | [2503.16470](http://arxiv.org/abs/2503.16470) |  | 20 pages, 5 figures | <details><summary>Abstract (click to expand)</summary>We model the time evolution of single win odds in Japanese horse racing as a stochastic process, deriving an Ornstein--Uhlenbeck process by analyzing the probability dynamics of vote shares and the empirical time series of odds movements. Our framework incorporates two types of bettors: herders, who adjust their bets based on current odds, and fundamentalists, who wager based on a horse's true winning probability. Using data from 3450 Japan Racing Association races in 2008, we identify a microscopic probability rule governing individual bets and a mean-reverting macroscopic pattern in odds convergence. This structure parallels financial markets, where traders' decisions are influenced by market fluctuations, and the interplay between herding and fundamentalist strategies shapes price dynamics. These results highlight the broader applicability of our approach to non-equilibrium financial and betting markets, where mean-reverting dynamics emerge from simple behavioral interactions.</details> |
| 2025-03-19 | HQNN-FSP: A Hybrid Classical-Quantum Neural Network for Regression-Based Financial Stock Market Prediction | Prashant Kumar Choudhary, Nouhaila Innan, Muhammad Shafique et.al. | [2503.15403](http://arxiv.org/abs/2503.15403) |  | 11 pages and 11 figures | <details><summary>Abstract (click to expand)</summary>Financial time-series forecasting remains a challenging task due to complex temporal dependencies and market fluctuations. This study explores the potential of hybrid quantum-classical approaches to assist in financial trend prediction by leveraging quantum resources for improved feature representation and learning. A custom Quantum Neural Network (QNN) regressor is introduced, designed with a novel ansatz tailored for financial applications. Two hybrid optimization strategies are proposed: (1) a sequential approach where classical recurrent models (RNN/LSTM) extract temporal dependencies before quantum processing, and (2) a joint learning framework that optimizes classical and quantum parameters simultaneously. Systematic evaluation using TimeSeriesSplit, k-fold cross-validation, and predictive error analysis highlights the ability of these hybrid models to integrate quantum computing into financial forecasting workflows. The findings demonstrate how quantum-assisted learning can contribute to financial modeling, offering insights into the practical role of quantum resources in time-series analysis.</details> |
| 2025-03-18 | A Note on the Asymptotic Properties of the GLS Estimator in Multivariate Regression with Heteroskedastic and Autocorrelated Errors | Koichiro Moriya, Akihiko Noda et.al. | [2503.13950](http://arxiv.org/abs/2503.13950) |  | 10 pages, 2 tables | <details><summary>Abstract (click to expand)</summary>We study the asymptotic properties of the GLS estimator in multivariate regression with heteroskedastic and autocorrelated errors. We derive Wald statistics for linear restrictions and assess their performance. The statistics remains robust to heteroskedasticity and autocorrelation.</details> |
| 2025-03-06 | Matrix H-theory approach to stock market fluctuations | Luan M. T. de Moraes, Ant√¥nio M. S. Macedo, Raydonal Ospina et.al. | [2503.08697](http://arxiv.org/abs/2503.08697) |  | 26 pages, 10 figures. Published on Physical Review E | <details><summary>Abstract (click to expand)</summary>We introduce matrix H theory, a framework for analyzing collective behavior arising from multivariate stochastic processes with hierarchical structure. The theory models the joint distribution of the multiple variables (the measured signal) as a compound of a large-scale multivariate distribution with the distribution of a slowly fluctuating background. The background is characterized by a hierarchical stochastic evolution of internal degrees of freedom, representing the correlations between stocks at different time scales. As in its univariate version, the matrix H-theory formalism also has two universality classes: Wishart and inverse Wishart, enabling a concise description of both the background and the signal probability distributions in terms of Meijer G-functions with matrix argument. Empirical analysis of daily returns of stocks within the S&P500 demonstrates the effectiveness of matrix H theory in describing fluctuations in stock markets. These findings contribute to a deeper understanding of multivariate hierarchical processes and offer potential for developing more informed portfolio strategies in financial markets.</details> |
| 2025-03-05 | Multimodal Stock Price Prediction: A Case Study of the Russian Securities Market | Kasymkhan Khubiev, Mikhail Semenov et.al. | [2503.08696](http://arxiv.org/abs/2503.08696) |  | NSCF-2024, PROGRAM SYSTEMS: THEORY AND APPLICATIONS | <details><summary>Abstract (click to expand)</summary>Classical asset price forecasting methods primarily rely on numerical data, such as price time series, trading volumes, limit order book data, and technical analysis indicators. However, the news flow plays a significant role in price formation, making the development of multimodal approaches that combine textual and numerical data for improved prediction accuracy highly relevant. This paper addresses the problem of forecasting financial asset prices using the multimodal approach that combines candlestick time series and textual news flow data. A unique dataset was collected for the study, which includes time series for 176 Russian stocks traded on the Moscow Exchange and 79,555 financial news articles in Russian. For processing textual data, pre-trained models RuBERT and Vikhr-Qwen2.5-0.5b-Instruct (a large language model) were used, while time series and vectorized text data were processed using an LSTM recurrent neural network. The experiments compared models based on a single modality (time series only) and two modalities, as well as various methods for aggregating text vector representations. Prediction quality was estimated using two key metrics: Accuracy (direction of price movement prediction: up or down) and Mean Absolute Percentage Error (MAPE), which measures the deviation of the predicted price from the true price. The experiments showed that incorporating textual modality reduced the MAPE value by 55%. The resulting multimodal dataset holds value for the further adaptation of language models in the financial sector. Future research directions include optimizing textual modality parameters, such as the time window, sentiment, and chronological order of news messages.</details> |
| 2025-03-02 | Liquidity-adjusted Return and Volatility, and Autoregressive Models | Qi Deng, Zhong-guo Zhou et.al. | [2503.08693](http://arxiv.org/abs/2503.08693) |  |  | <details><summary>Abstract (click to expand)</summary>We construct liquidity-adjusted return and volatility using purposely designed liquidity metrics (liquidity jump and liquidity diffusion) that incorporate additional liquidity information. Based on these measures, we introduce a liquidity-adjusted ARMA-GARCH framework to address the limitations of traditional ARMA-GARCH models, which are not effectively in modeling illiquid assets with high liquidity variability, such as cryptocurrencies. We demonstrate that the liquidity-adjusted model improves model fit for cryptocurrencies, with greater volatility sensitivity to past shocks and reduced volatility persistence of erratic past volatility. Our model is validated by the empirical evidence that the liquidity-adjusted mean-variance (LAMV) portfolios outperform the traditional mean-variance (TMV) portfolios.</details> |
| 2025-02-27 | Detecting Crypto Pump-and-Dump Schemes: A Thresholding-Based Approach to Handling Market Noise | Mahya Karbalaii et.al. | [2503.08692](http://arxiv.org/abs/2503.08692) |  |  | <details><summary>Abstract (click to expand)</summary>We propose a simple yet robust unsupervised model to detect pump-and-dump events on tokens listed on the Poloniex Exchange platform. By combining threshold-based criteria with exponentially weighted moving averages (EWMA) and volatility measures, our approach effectively distinguishes genuine anomalies from minor trading fluctuations, even for tokens with low liquidity and prolonged inactivity. These characteristics present a unique challenge, as standard anomaly-detection methods often over-flag negligible volume spikes. Our framework overcomes this issue by tailoring both price and volume thresholds to the specific trading patterns observed, resulting in a model that balances high true-positive detection with minimal noise.</details> |
| 2025-03-18 | Large language models in finance : what is financial sentiment? | Kemal Kirtac, Guido Germano et.al. | [2503.03612](http://arxiv.org/abs/2503.03612) |  | There are two different articles with the same content and different   names (see arXiv:2412.19245) | <details><summary>Abstract (click to expand)</summary>Financial sentiment has become a crucial yet complex concept in finance, increasingly used in market forecasting and investment strategies. Despite its growing importance, there remains a need to define and understand what financial sentiment truly represents and how it can be effectively measured. We explore the nature of financial sentiment and investigate how large language models (LLMs) contribute to its estimation. We trace the evolution of sentiment measurement in finance, from market-based and lexicon-based methods to advanced natural language processing techniques. The emergence of LLMs has significantly enhanced sentiment analysis, providing deeper contextual understanding and greater accuracy in extracting sentiment from financial text. We examine how BERT-based models, such as RoBERTa and FinBERT, are optimized for structured sentiment classification, while GPT-based models, including GPT-4, OPT, and LLaMA, excel in financial text generation and real-time sentiment interpretation. A comparative analysis of bidirectional and autoregressive transformer architectures highlights their respective roles in investor sentiment analysis, algorithmic trading, and financial decision-making. By exploring what financial sentiment is and how it is estimated within LLMs, we provide insights into the growing role of AI-driven sentiment analysis in finance.</details> |
| 2025-03-04 | VWAP Execution with Signature-Enhanced Transformers: A Multi-Asset Learning Approach | Remi Genet et.al. | [2503.02680](http://arxiv.org/abs/2503.02680) | **[link](https://github.com/remigenet/DynamicVWAPTransformer)** |  | <details><summary>Abstract (click to expand)</summary>In this paper I propose a novel approach to Volume Weighted Average Price (VWAP) execution that addresses two key practical challenges: the need for asset-specific model training and the capture of complex temporal dependencies. Building upon my recent work in dynamic VWAP execution arXiv:2502.18177, I demonstrate that a single neural network trained across multiple assets can achieve performance comparable to or better than traditional asset-specific models. The proposed architecture combines a transformer-based design inspired by arXiv:2406.02486 with path signatures for capturing geometric features of price-volume trajectories, as in arXiv:2406.17890. The empirical analysis, conducted on hourly cryptocurrency trading data from 80 trading pairs, shows that the globally-fitted model with signature features (GFT-Sig) achieves superior performance in both absolute and quadratic VWAP loss metrics compared to asset-specific approaches. Notably, these improvements persist for out-of-sample assets, demonstrating the model's ability to generalize across different market conditions. The results suggest that combining global parameter sharing with signature-based feature extraction provides a scalable and robust approach to VWAP execution, offering significant practical advantages over traditional asset-specific implementations.</details> |
| 2025-03-04 | Extrapolating the long-term seasonal component of electricity prices for forecasting in the day-ahead market | Katarzyna Chƒôƒá, Bartosz Uniejewski, Rafa≈Ç Weron et.al. | [2503.02518](http://arxiv.org/abs/2503.02518) |  |  | <details><summary>Abstract (click to expand)</summary>Recent studies provide evidence that decomposing the electricity price into the long-term seasonal component (LTSC) and the remaining part, predicting both separately, and then combining their forecasts can bring significant accuracy gains in day-ahead electricity price forecasting. However, not much attention has been paid to predicting the LTSC, and the last 24 hourly values of the estimated pattern are typically copied for the target day. To address this gap, we introduce a novel approach which extracts the trend-seasonal pattern from a price series extrapolated using price forecasts for the next 24 hours. We assess it using two 5-year long test periods from the German and Spanish power markets, covering the Covid-19 pandemic, the 2021/2022 energy crisis, and the war in Ukraine. Considering parsimonious autoregressive and LASSO-estimated models, we find that improvements in predictive accuracy range from 3\% to 15\% in terms of the root mean squared error and exceed 1\% in terms of profits from a realistic trading strategy involving day-ahead bidding and battery storage.</details> |
| 2025-03-01 | Understanding the Commodity Futures Term Structure Through Signatures | Hari P. Krishnan, Stephan Sturm et.al. | [2503.00603](http://arxiv.org/abs/2503.00603) |  | 19 pages, 1 figure | <details><summary>Abstract (click to expand)</summary>Signature methods have been widely and effectively used as a tool for feature extraction in statistical learning methods, notably in mathematical finance. They lack, however, interpretability: in the general case, it is unclear why signatures actually work. The present article aims to address this issue directly, by introducing and developing the concept of signature perturbations. In particular, we construct a regular perturbation of the signature of the term structure of log prices for various commodities, in terms of the convenience yield. Our perturbation expansion and rigorous convergence estimates help explain the success of signature-based classification of commodities markets according to their term structure, with the volatility of the convenience yield as the major discriminant.</details> |
| 2025-03-04 | Using quantile time series and historical simulation to forecast financial risk multiple steps ahead | Richard Gerlach, Antonio Naimoli, Giuseppe Storti et.al. | [2502.20978](http://arxiv.org/abs/2502.20978) |  |  | <details><summary>Abstract (click to expand)</summary>A method for quantile-based, semi-parametric historical simulation estimation of multiple step ahead Value-at-Risk (VaR) and Expected Shortfall (ES) models is developed. It uses the quantile loss function, analogous to how the quasi-likelihood is employed by standard historical simulation methods. The returns data are scaled by the estimated quantile series, then resampling is employed to estimate the forecast distribution one and multiple steps ahead, allowing tail risk forecasting. The proposed method is applicable to any data or model where the relationship between VaR and ES does not change over time and can be extended to allow a measurement equation incorporating realized measures, thus including Realized GARCH and Realized CAViaR type models. Its finite sample properties, and its comparison with existing historical simulation methods, are evaluated via a simulation study. A forecasting study assesses the relative accuracy of the 1% and 2.5% VaR and ES one-day-ahead and ten-day-ahead forecasting results for the proposed class of models compared to several competitors.</details> |
| 2025-02-26 | Corporate Fraud Detection in Rich-yet-Noisy Financial Graph | Shiqi Wang, Zhibo Zhang, Libing Fang et.al. | [2502.19305](http://arxiv.org/abs/2502.19305) | **[link](https://github.com/wangskyGit/KeHGN-R)** |  | <details><summary>Abstract (click to expand)</summary>Corporate fraud detection aims to automatically recognize companies that conduct wrongful activities such as fraudulent financial statements or illegal insider trading. Previous learning-based methods fail to effectively integrate rich interactions in the company network. To close this gap, we collect 18-year financial records in China to form three graph datasets with fraud labels. We analyze the characteristics of the financial graphs, highlighting two pronounced issues: (1) information overload: the dominance of (noisy) non-company nodes over company nodes hinders the message-passing process in Graph Convolution Networks (GCN); and (2) hidden fraud: there exists a large percentage of possible undetected violations in the collected data. The hidden fraud problem will introduce noisy labels in the training dataset and compromise fraud detection results. To handle such challenges, we propose a novel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage Learning ( ${\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to mitigate the information overload and effectively learns rich representations. The proposed model adopts a two-stage learning method to enhance robustness against hidden frauds. Extensive experimental results not only confirm the importance of interactions but also show the superiority of ${\rm KeGCN}_{R}$ over a number of strong baselines in terms of fraud detection effectiveness and robustness.</details> |
| 2025-02-25 | Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading Strategies with Temporal Kolmogorov-Arnold Networks | Remi Genet et.al. | [2502.18177](http://arxiv.org/abs/2502.18177) | **[link](https://github.com/remigenet/deepdynamicvwap)** |  | <details><summary>Abstract (click to expand)</summary>The execution of Volume Weighted Average Price (VWAP) orders remains a critical challenge in modern financial markets, particularly as trading volumes and market complexity continue to increase. In my previous work arXiv:2502.13722, I introduced a novel deep learning approach that demonstrated significant improvements over traditional VWAP execution methods by directly optimizing the execution problem rather than relying on volume curve predictions. However, that model was static because it employed the fully linear approach described in arXiv:2410.21448, which is not designed for dynamic adjustment. This paper extends that foundation by developing a dynamic neural VWAP framework that adapts to evolving market conditions in real time. We introduce two key innovations: first, the integration of recurrent neural networks to capture complex temporal dependencies in market dynamics, and second, a sophisticated dynamic adjustment mechanism that continuously optimizes execution decisions based on market feedback. The empirical analysis, conducted across five major cryptocurrency markets, demonstrates that this dynamic approach achieves substantial improvements over both traditional methods and our previous static implementation, with execution performance gains of 10 to 15% in liquid markets and consistent outperformance across varying conditions. These results suggest that adaptive neural architectures can effectively address the challenges of modern VWAP execution while maintaining computational efficiency suitable for practical deployment.</details> |
| 2025-02-25 | LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena | Tianmi Ma, Jiawei Du, Wenxin Huang et.al. | [2502.17967](http://arxiv.org/abs/2502.17967) | **[link](https://github.com/wekjsdvnm/agent-trading-arena)** |  | <details><summary>Abstract (click to expand)</summary>Recent advancements in large language models (LLMs) have significantly improved performance in natural language processing tasks. However, their ability to generalize to dynamic, unseen tasks, particularly in numerical reasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on problems with predefined optimal solutions, which may not align with real-world scenarios where clear answers are absent. To bridge this gap, we design the Agent Trading Arena, a virtual numerical game simulating complex economic systems through zero-sum games, where agents invest in stock portfolios. Our experiments reveal that LLMs, including GPT-4o, struggle with algebraic reasoning when dealing with plain-text stock data, often focusing on local details rather than global trends. In contrast, LLMs perform significantly better with geometric reasoning when presented with visual data, such as scatter plots or K-line charts, suggesting that visual representations enhance numerical reasoning. This capability is further improved by incorporating the reflection module, which aids in the analysis and interpretation of complex data. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate stronger reasoning with visual data compared to text. Our code and data are publicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.</details> |
| 2025-02-24 | A data-driven econo-financial stress-testing framework to estimate the effect of supply chain networks on financial systemic risk | Jan Fialkowski, Christian Diem, Andr√°s Borsos et.al. | [2502.17044](http://arxiv.org/abs/2502.17044) | **[link](https://github.com/JanFialkowski/FSRI_Plus)** |  | <details><summary>Abstract (click to expand)</summary>Supply chain disruptions constitute an often underestimated risk for financial stability. As in financial networks, systemic risks in production networks arises when the local failure of one firm impacts the production of others and might trigger cascading disruptions that affect significant parts of the economy. Here, we study how systemic risk in production networks translates into financial systemic risk through a mechanism where supply chain contagion leads to correlated bank-firm loan defaults. We propose a financial stress-testing framework for micro- and macro-prudential applications that features a national firm level supply chain network in combination with interbank network layers. The model is calibrated by using a unique data set including about 1 million firm-level supply links, practically all bank-firm loans, and all interbank loans in a small European economy. As a showcase we implement a real COVID-19 shock scenario on the firm level. This model allows us to study how the disruption dynamics in the real economy can lead to interbank solvency contagion dynamics. We estimate to what extent this amplifies financial systemic risk. We discuss the relative importance of these contagion channels and find an increase of interbank contagion by 70% when production network contagion is present. We then examine the financial systemic risk firms bring to banks and find an increase of up to 28% in the presence of the interbank contagion channel. This framework is the first financial systemic risk model to take agent-level dynamics of the production network and shocks of the real economy into account which opens a path for directly, and event-driven understanding of the dynamical interaction between the real economy and financial systems.</details> |
| 2025-02-22 | Contrastive Similarity Learning for Market Forecasting: The ContraSim Framework | Nicholas Vinden, Raeid Saqur, Zining Zhu et.al. | [2502.16023](http://arxiv.org/abs/2502.16023) |  | 8 pages, 3 appendices | <details><summary>Abstract (click to expand)</summary>We introduce the Contrastive Similarity Space Embedding Algorithm (ContraSim), a novel framework for uncovering the global semantic relationships between daily financial headlines and market movements. ContraSim operates in two key stages: (I) Weighted Headline Augmentation, which generates augmented financial headlines along with a semantic fine-grained similarity score, and (II) Weighted Self-Supervised Contrastive Learning (WSSCL), an extended version of classical self-supervised contrastive learning that uses the similarity metric to create a refined weighted embedding space. This embedding space clusters semantically similar headlines together, facilitating deeper market insights. Empirical results demonstrate that integrating ContraSim features into financial forecasting tasks improves classification accuracy from WSJ headlines by 7%. Moreover, leveraging an information density analysis, we find that the similarity spaces constructed by ContraSim intrinsically cluster days with homogeneous market movement directions, indicating that ContraSim captures market dynamics independent of ground truth labels. Additionally, ContraSim enables the identification of historical news days that closely resemble the headlines of the current day, providing analysts with actionable insights to predict market trends by referencing analogous past events.</details> |
| 2025-02-21 | Multi-Agent Stock Prediction Systems: Machine Learning Models, Simulations, and Real-Time Trading Strategies | Daksh Dave, Gauransh Sawhney, Vikhyat Chauhan et.al. | [2502.15853](http://arxiv.org/abs/2502.15853) |  |  | <details><summary>Abstract (click to expand)</summary>This paper presents a comprehensive study on stock price prediction, leveragingadvanced machine learning (ML) and deep learning (DL) techniques to improve financial forecasting accuracy. The research evaluates the performance of various recurrent neural network (RNN) architectures, including Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), and attention-based models. These models are assessed for their ability to capture complex temporal dependencies inherent in stock market data. Our findings show that attention-based models outperform other architectures, achieving the highest accuracy by capturing both short and long-term dependencies. This study contributes valuable insights into AI-driven financial forecasting, offering practical guidance for developing more accurate and efficient trading systems.</details> |
| 2025-02-20 | Financial fraud detection system based on improved random forest and gradient boosting machine (GBM) | Tianzuo Hu et.al. | [2502.15822](http://arxiv.org/abs/2502.15822) |  |  | <details><summary>Abstract (click to expand)</summary>This paper proposes a financial fraud detection system based on improved Random Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the system introduces a novel model architecture called GBM-SSRF (Gradient Boosting Machine with Simplified and Strengthened Random Forest), which cleverly combines the powerful optimization capabilities of the gradient boosting machine (GBM) with improved randomization. The computational efficiency and feature extraction capabilities of the Simplified and Strengthened Random Forest (SSRF) forest significantly improve the performance of financial fraud detection. Although the traditional random forest model has good classification capabilities, it has high computational complexity when faced with large-scale data and has certain limitations in feature selection. As a commonly used ensemble learning method, the GBM model has significant advantages in optimizing performance and handling nonlinear problems. However, GBM takes a long time to train and is prone to overfitting problems when data samples are unbalanced. In response to these limitations, this paper optimizes the random forest based on the structure, reducing the computational complexity and improving the feature selection ability through the structural simplification and enhancement of the random forest. In addition, the optimized random forest is embedded into the GBM framework, and the model can maintain efficiency and stability with the help of GBM's gradient optimization capability. Experiments show that the GBM-SSRF model not only has good performance, but also has good robustness and generalization capabilities, providing an efficient and reliable solution for financial fraud detection.</details> |
| 2025-02-21 | Network topology of the Euro Area interbank market | Ilias Aarab, Thomas Gottron et.al. | [2502.15611](http://arxiv.org/abs/2502.15611) |  | This is the preprint version of the paper published in: Aarab, I.,   Gottron, T. (2024). Network Topology of the Euro Area Interbank Market. In:   Mingione, M., Vichi, M., Zaccaria, G. (eds) *High-quality and Timely   Statistics*. CESS 2022. Studies in Theoretical and Applied Statistics.   Springer, Cham. <https://doi.org/10.1007/978-3-031-63630-1_1> | <details><summary>Abstract (click to expand)</summary>The rapidly increasing availability of large amounts of granular financial data, paired with the advances of big data related technologies induces the need of suitable analytics that can represent and extract meaningful information from such data. In this paper we propose a multi-layer network approach to distill the Euro Area (EA) banking system in different distinct layers. Each layer of the network represents a specific type of financial relationship between banks, based on various sources of EA granular data collections. The resulting multi-layer network allows one to describe, analyze and compare the topology and structure of EA banks from different perspectives, eventually yielding a more complete picture of the financial market. This granular information representation has the potential to enable researchers and practitioners to better apprehend financial system dynamics as well as to support financial policies to manage and monitor financial risk from a more holistic point of view.</details> |
| 2025-02-21 | Clustered Network Connectedness: A New Measurement Framework with Application to Global Equity Markets | Bastien Buchwalter, Francis X. Diebold, Kamil Yilmaz et.al. | [2502.15458](http://arxiv.org/abs/2502.15458) |  |  | <details><summary>Abstract (click to expand)</summary>Network connections, both across and within markets, are central in countless economic contexts. In recent decades, a large literature has developed and applied flexible methods for measuring network connectedness and its evolution, based on variance decompositions from vector autoregressions (VARs), as in Diebold and Yilmaz (2014). Those VARs are, however, typically identified using full orthogonalization (Sims, 1980), or no orthogonalization (Koop, Pesaran, and Potter, 1996; Pesaran and Shin, 1998), which, although useful, are special and extreme cases of a more general framework that we develop in this paper. In particular, we allow network nodes to be connected in "clusters", such as asset classes, industries, regions, etc., where shocks are orthogonal across clusters (Sims style orthogonalized identification) but correlated within clusters (Koop-Pesaran-Potter-Shin style generalized identification), so that the ordering of network nodes is relevant across clusters but irrelevant within clusters. After developing the clustered connectedness framework, we apply it in a detailed empirical exploration of sixteen country equity markets spanning three global regions.</details> |
| 2025-02-20 | Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework | Arno Botha, Tanja Verster, Roland Breedt et.al. | [2502.14479](http://arxiv.org/abs/2502.14479) |  | 33 pages, 8192 words, 12 figures | <details><summary>Abstract (click to expand)</summary>The lifetime behaviour of loans is notoriously difficult to model, which can compromise a bank's financial reserves against future losses, if modelled poorly. Therefore, we present a data-driven comparative study amongst three techniques in modelling a series of default risk estimates over the lifetime of each loan, i.e., its term-structure. The behaviour of loans can be described using a nonstationary and time-dependent semi-Markov model, though we model its elements using a multistate regression-based approach. As such, the transition probabilities are explicitly modelled as a function of a rich set of input variables, including macroeconomic and loan-level inputs. Our modelling techniques are deliberately chosen in ascending order of complexity: 1) a Markov chain; 2) beta regression; and 3) multinomial logistic regression. Using residential mortgage data, our results show that each successive model outperforms the previous, likely as a result of greater sophistication. This finding required devising a novel suite of simple model diagnostics, which can itself be reused in assessing sampling representativeness and the performance of other modelling techniques. These contributions surely advance the current practice within banking when conducting multistate modelling. Consequently, we believe that the estimation of loss reserves will be more timeous and accurate under IFRS 9.</details> |
| 2025-02-20 | Causality Analysis of COVID-19 Induced Crashes in Stock and Commodity Markets: A Topological Perspective | Buddha Nath Sharma, Anish Rai, SR Luwang et.al. | [2502.14431](http://arxiv.org/abs/2502.14431) |  |  | <details><summary>Abstract (click to expand)</summary>The paper presents a comprehensive causality analysis of the US stock and commodity markets during the COVID-19 crash. The dynamics of different sectors are also compared. We use Topological Data Analysis (TDA) on multidimensional time-series to identify crashes in stock and commodity markets. The Wasserstein Distance WD shows distinct spikes signaling the crash for both stock and commodity markets. We then compare the persistence diagrams of stock and commodity markets using the WD metric. A significant spike in the $WD$ between stock and commodity markets is observed during the crisis, suggesting significant topological differences between the markets. Similar spikes are observed between the sectors of the US market as well. Spikes obtained may be due to either a difference in the magnitude of crashes in the two markets (or sectors), or from the temporal lag between the two markets suggesting information flow. We study the Granger-causality between stock and commodity markets and also between different sectors. The results show a bidirectional Granger-causality between commodity and stock during the crash period, demonstrating the greater interdependence of financial markets during the crash. However, the overall analysis shows that the causal direction is from stock to commodity. A pairwise Granger-causal analysis between US sectors is also conducted. There is a significant increase in the interdependence between the sectors during the crash period. TDA combined with Granger-causality effectively analyzes the interdependence and sensitivity of different markets and sectors.</details> |

<p align=right>(<a href=#-updated-on-20250617>back to top</a>)</p>

## üìå Deep Learning in Finance

| üìÖ Publish Date | üìñ Title | üë®‚Äçüíª Authors | üîó PDF | üíª Code | üí¨ Comment | üìú Abstract |
|:--------------:|:----------------------------|:------------------|:------:|:------:|:-------:|:--------|
| 2025-06-13 | CLEAN-MI: A Scalable and Efficient Pipeline for Constructing High-Quality Neurodata in Motor Imagery Paradigm | Dingkun Liu, Zhu Chen, Dongrui Wu et.al. | [2506.11830](http://arxiv.org/abs/2506.11830) |  | 10 pages, 6 figures | <details><summary>Abstract (click to expand)</summary>The construction of large-scale, high-quality datasets is a fundamental prerequisite for developing robust and generalizable foundation models in motor imagery (MI)-based brain-computer interfaces (BCIs). However, EEG signals collected from different subjects and devices are often plagued by low signal-to-noise ratio, heterogeneity in electrode configurations, and substantial inter-subject variability, posing significant challenges for effective model training. In this paper, we propose CLEAN-MI, a scalable and systematic data construction pipeline for constructing large-scale, efficient, and accurate neurodata in the MI paradigm. CLEAN-MI integrates frequency band filtering, channel template selection, subject screening, and marginal distribution alignment to systematically filter out irrelevant or low-quality data and standardize multi-source EEG datasets. We demonstrate the effectiveness of CLEAN-MI on multiple public MI datasets, achieving consistent improvements in data quality and classification performance.</details> |
| 2025-06-13 | On the performance of multi-fidelity and reduced-dimensional neural emulators for inference of physiologic boundary conditions | Chloe H. Choi, Andrea Zanoni, Daniele E. Schiavazzi et.al. | [2506.11683](http://arxiv.org/abs/2506.11683) |  |  | <details><summary>Abstract (click to expand)</summary>Solving inverse problems in cardiovascular modeling is particularly challenging due to the high computational cost of running high-fidelity simulations. In this work, we focus on Bayesian parameter estimation and explore different methods to reduce the computational cost of sampling from the posterior distribution by leveraging low-fidelity approximations. A common approach is to construct a surrogate model for the high-fidelity simulation itself. Another is to build a surrogate for the discrepancy between high- and low-fidelity models. This discrepancy, which is often easier to approximate, is modeled with either a fully connected neural network or a nonlinear dimensionality reduction technique that enables surrogate construction in a lower-dimensional space. A third possible approach is to treat the discrepancy between the high-fidelity and surrogate models as random noise and estimate its distribution using normalizing flows. This allows us to incorporate the approximation error into the Bayesian inverse problem by modifying the likelihood function. We validate five different methods which are variations of the above on analytical test cases by comparing them to posterior distributions derived solely from high-fidelity models, assessing both accuracy and computational cost. Finally, we demonstrate our approaches on two cardiovascular examples of increasing complexity: a lumped-parameter Windkessel model and a patient-specific three-dimensional anatomy.</details> |
| 2025-06-13 | PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design | Zhenqiao Song, Tiaoxiao Li, Lei Li et.al. | [2506.11420](http://arxiv.org/abs/2506.11420) |  |  | <details><summary>Abstract (click to expand)</summary>Designing protein-binding proteins with high affinity is critical in biomedical research and biotechnology. Despite recent advancements targeting specific proteins, the ability to create high-affinity binders for arbitrary protein targets on demand, without extensive rounds of wet-lab testing, remains a significant challenge. Here, we introduce PPDiff, a diffusion model to jointly design the sequence and structure of binders for arbitrary protein targets in a non-autoregressive manner. PPDiffbuilds upon our developed Sequence Structure Interleaving Network with Causal attention layers (SSINC), which integrates interleaved self-attention layers to capture global amino acid correlations, k-nearest neighbor (kNN) equivariant graph layers to model local interactions in three-dimensional (3D) space, and causal attention layers to simplify the intricate interdependencies within the protein sequence. To assess PPDiff, we curate PPBench, a general protein-protein complex dataset comprising 706,360 complexes from the Protein Data Bank (PDB). The model is pretrained on PPBenchand finetuned on two real-world applications: target-protein mini-binder complex design and antigen-antibody complex design. PPDiffconsistently surpasses baseline methods, achieving success rates of 50.00%, 23.16%, and 16.89% for the pretraining task and the two downstream applications, respectively.</details> |
| 2025-06-12 | An Attention-based Spatio-Temporal Neural Operator for Evolving Physics | Vispi Karkaria, Doksoo Lee, Yi-Ping Chen et.al. | [2506.11328](http://arxiv.org/abs/2506.11328) |  |  | <details><summary>Abstract (click to expand)</summary>In scientific machine learning (SciML), a key challenge is learning unknown, evolving physical processes and making predictions across spatio-temporal scales. For example, in real-world manufacturing problems like additive manufacturing, users adjust known machine settings while unknown environmental parameters simultaneously fluctuate. To make reliable predictions, it is desired for a model to not only capture long-range spatio-temporal interactions from data but also adapt to new and unknown environments; traditional machine learning models excel at the first task but often lack physical interpretability and struggle to generalize under varying environmental conditions. To tackle these challenges, we propose the Attention-based Spatio-Temporal Neural Operator (ASNO), a novel architecture that combines separable attention mechanisms for spatial and temporal interactions and adapts to unseen physical parameters. Inspired by the backward differentiation formula (BDF), ASNO learns a transformer for temporal prediction and extrapolation and an attention-based neural operator for handling varying external loads, enhancing interpretability by isolating historical state contributions and external forces, enabling the discovery of underlying physical laws and generalizability to unseen physical environments. Empirical results on SciML benchmarks demonstrate that ASNO outperforms over existing models, establishing its potential for engineering applications, physics discovery, and interpretable machine learning.</details> |
| 2025-06-12 | Spectral Analysis of Discretized Boundary Integral Operators in 3D: a High-Frequency Perspective | V. Giunzioni, A. Merlini, F. P. Andriulli et.al. | [2506.10880](http://arxiv.org/abs/2506.10880) |  |  | <details><summary>Abstract (click to expand)</summary>When modeling propagation and scattering phenomena using integral equations discretized by the boundary element method, it is common practice to approximate the boundary of the scatterer with a mesh comprising elements of size approximately equal to a fraction of the wavelength $\lambda$ of the incident wave, e.g., $\lambda/10$ . In this work, by analyzing the spectra of the operator matrices, we show a discrepancy with respect to the continuous operators which grows with the simulation frequency, challenging the common belief that the aforementioned widely used discretization approach is sufficient to maintain the accuracy of the solution constant when increasing the frequency.</details> |
| 2025-06-13 | PDESpectralRefiner: Achieving More Accurate Long Rollouts with Spectral Adjustment | Li Luo et.al. | [2506.10711](http://arxiv.org/abs/2506.10711) |  |  | <details><summary>Abstract (click to expand)</summary>Generating accurate and stable long rollouts is a notorious challenge for time-dependent PDEs (Partial Differential Equations). Recently, motivated by the importance of high-frequency accuracy, a refiner model called PDERefiner utilizes diffusion models to refine outputs for every time step, since the denoising process could increase the correctness of modeling high frequency part. For 1-D Kuramoto-Sivashinsky equation, refiner models can degrade the amplitude of high frequency part better than not doing refinement process. However, for some other cases, the spectrum might be more complicated. For example, for a harder PDE like Navior-Stokes equation, diffusion models could over-degrade the higher frequency part. This motivates us to release the constraint that each frequency weighs the same. We enhance our refiner model with doing adjustments on spectral space, which recovers Blurring diffusion models. We developed a new v-prediction technique for Blurring diffusion models, recovering the MSE training objective on the first refinement step. We show that in this case, for different model backbones, such as U-Net and neural operators, the outputs of PDE-SpectralRefiner are more accurate for both one-step MSE loss and rollout loss.</details> |
| 2025-06-11 | Interpretable and flexible non-intrusive reduced-order models using reproducing kernel Hilbert spaces | Alejandro N Diaz, Shane A McQuarrie, John T Tencer et.al. | [2506.10224](http://arxiv.org/abs/2506.10224) |  |  | <details><summary>Abstract (click to expand)</summary>This paper develops an interpretable, non-intrusive reduced-order modeling technique using regularized kernel interpolation. Existing non-intrusive approaches approximate the dynamics of a reduced-order model (ROM) by solving a data-driven least-squares regression problem for low-dimensional matrix operators. Our approach instead leverages regularized kernel interpolation, which yields an optimal approximation of the ROM dynamics from a user-defined reproducing kernel Hilbert space. We show that our kernel-based approach can produce interpretable ROMs whose structure mirrors full-order model structure by embedding judiciously chosen feature maps into the kernel. The approach is flexible and allows a combination of informed structure through feature maps and closure terms via more general nonlinear terms in the kernel. We also derive a computable a posteriori error bound that combines standard error estimates for intrusive projection-based ROMs and kernel interpolants. The approach is demonstrated in several numerical experiments that include comparisons to operator inference using both proper orthogonal decomposition and quadratic manifold dimension reduction.</details> |
| 2025-06-11 | Exploring EEG Responses during Observation of Actions Performed by Human Actor and Humanoid Robot | Anh T. Nguyen, Ajay Anand, Michelle J. Johnson et.al. | [2506.10170](http://arxiv.org/abs/2506.10170) |  |  | <details><summary>Abstract (click to expand)</summary>Action observation (AO) therapy is a promising rehabilitative treatment for motor and language function in individuals recovering from neurological conditions, such as stroke. This pilot study aimed to investigate the potential of humanoid robots to support AO therapy in rehabilitation settings. The brain activity of three healthy right-handed participants was monitored with electroencephalography (EEG) while they observed eight different actions performed by two agents, a human actor and a robot, using their left and right arms. Their event-related spectral perturbations (ERSPs, changes in the spectral power of neural oscillations in response to an event or stimulus, compared to baseline) in sensorimotor regions were analyzed. The single-subject analysis showed variability in ERSP patterns among all participants, including power suppression in sensorimotor mu and beta rhythms. One participant showed stronger responses to "robot" AO conditions than to "human" conditions. Strong and positive correlations in ERSP across all conditions were observed for almost all participants and channels, implying common cognitive processes or neural networks at play in the mirror neuron system during AO. The results support the feasibility of using EEG to explore differences in neural responses to observation of robot- and human-induced actions.</details> |
| 2025-06-11 | Improving the performance of optical inverse design of multilayer thin films using CNN-LSTM tandem neural networks | Uijun Jung, Deokho Jang, Sungchul Kim et.al. | [2506.10044](http://arxiv.org/abs/2506.10044) |  | 22 pages, 8 figures, 2 tables, 11 supplementary figures, 7   supplementary tables | <details><summary>Abstract (click to expand)</summary>Optical properties of thin film are greatly influenced by the thickness of each layer. Accurately predicting these thicknesses and their corresponding optical properties is important in the optical inverse design of thin films. However, traditional inverse design methods usually demand extensive numerical simulations and optimization procedures, which are time-consuming. In this paper, we utilize deep learning for the inverse design of the transmission spectra of SiO2/TiO2 multilayer thin films. We implement a tandem neural network (TNN), which can solve the one-to-many mapping problem that greatly degrades the performance of deep-learning-based inverse designs. In general, the TNN has been implemented by a back-to-back connection of an inverse neural network and a pre-trained forward neural network, both of which have been implemented based on multilayer perceptron (MLP) algorithms. In this paper, we propose to use not only MLP, but also convolutional neural network (CNN) or long short-term memory (LSTM) algorithms in the configuration of the TNN. We show that an LSTM-LSTM-based TNN yields the highest accuracy but takes the longest training time among nine configurations of TNNs. We also find that a CNN-LSTM-based TNN will be an optimal solution in terms of accuracy and speed because it could integrate the strengths of the CNN and LSTM algorithms.</details> |
